{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import argparse, gzip, cPickle, sys, time, itertools\n",
    "\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "import autograd.scipy.stats.norm as norm\n",
    "import autograd.scipy.stats.dirichlet as dirichlet\n",
    "from autograd.scipy.misc import logsumexp\n",
    "from autograd.util import flatten_func, flatten\n",
    "from autograd import grad, primitive\n",
    "from autograd.numpy.numpy_grads import unbroadcast\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import mixture\n",
    "\n",
    "from  autograd.scipy.special import gammaln, digamma, gamma\n",
    "from scipy import linalg\n",
    "from scipy import stats, integrate\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.animation as animation\n",
    "import pandas as pd\n",
    "import six\n",
    "\n",
    "color_names =  [\"windows blue\",\"red\", \"gold\", \"grass green\", \"orange\",\n",
    "                \"yellow\", \"cornflower\", \"dark red\", \"dark blue\", \"brown\"]\n",
    "colors = sns.xkcd_palette(color_names)\n",
    "sns.set_style(\"white\")\n",
    "color_iter = itertools.cycle(colors)\n",
    "\n",
    "def plot_results(ax, X, Y, means, covariances, index, title):    \n",
    "    for i, (mean, covar, color) in enumerate(zip(\n",
    "             means, covariances, color_iter)):\n",
    "        v, w = linalg.eigh(np.diag(np.full([2], covar)))\n",
    "        v = 2. * np.sqrt(2.) * np.sqrt(v)\n",
    "        u = w[0] / linalg.norm(w[0])      \n",
    "\n",
    "        if not np.any(Y == i):\n",
    "            continue\n",
    "        ax.scatter(X[Y == i, 0], X[Y == i, 1], 2., color=color)\n",
    "\n",
    "        angle = np.arctan(u[1] / u[0])\n",
    "        angle = 180. * angle / np.pi  # convert to degrees\n",
    "        ell = mpl.patches.Ellipse(mean, v[0], v[1], 180. + angle, color=color)\n",
    "        #ell.set_clip_box(splot.bbox)\n",
    "        ell.set_alpha(0.5)\n",
    "        ax.add_artist(ell)\n",
    "        \n",
    "\n",
    "class Adam(object):\n",
    "    def __init__(self, dparam, b1=0.9, b2=0.999, eps=1,#10**-8,\n",
    "                         decay_rate = 0.9, decay_steps = 100):                    \n",
    "        self.b1 = b1;\n",
    "        self.b2 = b2;\n",
    "        self.eps = eps        \n",
    "        self.m = np.zeros(dparam)\n",
    "        self.v = np.zeros(dparam)\n",
    "        self.i = 0\n",
    "        self.decay_rate = decay_rate\n",
    "        self.decay_steps = decay_steps\n",
    "    \n",
    "    def update(self, gradients, params, learning_rate = 0.1):        \n",
    "        self.i = self.i+1\n",
    "        step_size = learning_rate * self.decay_rate**(self.i/self.decay_steps)        \n",
    "        self.m = (1 - self.b1) * gradients + self.b1 * self.m\n",
    "        self.v = (1 - self.b2) * (gradients**2) + self.b2 * self.v\n",
    "        mhat = self.m / (1 - self.b1**(self.i))\n",
    "        vhat = self.v / (1 - self.b2**(self.i))                        \n",
    "        params = params + step_size*mhat/(np.sqrt(vhat) + self.eps)        \n",
    "        return np.split(params,2)\n",
    "\n",
    "    \n",
    "@primitive\n",
    "def softplus(x):\n",
    "    return np.log(1. + np.exp(x))\n",
    "\n",
    "softplus.defvjp(lambda g, ans, vs, gvs, x: unbroadcast(vs, gvs, g * 1./(1. + np.exp(-x))))\n",
    "\n",
    "def jacobian_softplus(x):\n",
    "    return 1./(1. + np.exp(-x))\n",
    "\n",
    "@primitive\n",
    "def gamma_logpdf(x, alpha = 1., beta = 1.):\n",
    "    return  (alpha*np.log(beta) + (alpha - 1)*np.log(x) - x*beta - gammaln(alpha))\n",
    "\n",
    "gamma_logpdf.defvjp(lambda g, ans, vs, gvs, x, alpha=1.0, beta=1.0: unbroadcast(vs, gvs, g * ((alpha-1)/x - beta)))\n",
    "gamma_logpdf.defvjp(lambda g, ans, vs, gvs, x, alpha=1.0, beta=1.0: unbroadcast(vs, gvs,  g * (np.log(beta) + np.log(x) - digamma(alpha))), argnum=1)\n",
    "gamma_logpdf.defvjp(lambda g, ans, vs, gvs, x, alpha=1.0, beta=1.0: unbroadcast(vs, gvs,  g * (alpha/beta - x)), argnum=2)\n",
    "\n",
    "\n",
    "cumprod = primitive(np.cumprod)\n",
    "\n",
    "def grad_np_cumprod(g, ans, vs, gvs, x, axis=None):\n",
    "    fx = np.cumprod(x, axis=None)\n",
    "    return np.cumsum((fx * g)[::-1])[::-1].reshape(x.shape) / x\n",
    "\n",
    "cumprod.defvjp(grad_np_cumprod)\n",
    "\n",
    "def invlogit(x, eps=sys.float_info.epsilon):\n",
    "    return (1 - 2 * eps) / (1 + np.exp(-x)) + eps\n",
    "\n",
    "def logit(p):\n",
    "    return np.log(p / (1 - p))\n",
    "\n",
    "def stick_breaking(y_):\n",
    "    y = y_.T\n",
    "    Km1 = y.shape[0]\n",
    "    k = np.arange(Km1)[(slice(None), ) + (None, ) * (y.ndim - 1)]\n",
    "    eq_share = logit(1. / (Km1 + 1 - k).astype(str(y_.dtype)))\n",
    "    z = invlogit(y + eq_share, 1e-3)\n",
    "    yl = np.concatenate([z, np.ones(y[:1].shape)])\n",
    "    yu = np.concatenate([np.ones(y[:1].shape), 1 - z])\n",
    "    S = cumprod(yu, 0)\n",
    "    x = S * yl\n",
    "    return x.T\n",
    "\n",
    "def jacobian_stick_breaking(y_):\n",
    "    y = y_.T\n",
    "    Km1 = y.shape[0]\n",
    "    k = np.arange(Km1)[(slice(None), ) + (None, ) * (y.ndim - 1)]\n",
    "    eq_share = logit(1. / (Km1 + 1 - k).astype(str(y_.dtype)))\n",
    "    yl = y + eq_share\n",
    "    yu = np.concatenate([np.ones(y[:1].shape), 1 - invlogit(yl, 1e-3)])\n",
    "    S = cumprod(yu, 0)\n",
    "    return -np.sum(np.log(S[:-1]) - np.log1p(np.exp(yl)) - np.log1p(np.exp(-yl)), 0).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class GMM(object):\n",
    "    def __init__(self, data, clusters, scale):\n",
    "        self.data = data     \n",
    "        self.clusters = clusters\n",
    "        self.scale = scale\n",
    "        self.N = data.shape[0]\n",
    "        self.D = data.shape[1]        \n",
    "    \n",
    "    def p_log_prob(self, idx, z):\n",
    "        x = self.data[idx]        \n",
    "        mu, tau, pi = z['mu'], softplus(z['tau']), stick_breaking(z['pi'])\n",
    "        matrix = []  \n",
    "        log_prior = 0.\n",
    "        log_prior += np.sum(gamma_logpdf(tau, 1e-5, 1e-5) + np.log(jacobian_softplus(z['tau'])))        \n",
    "        log_prior += np.sum(norm.logpdf(mu, 0, 1.))\n",
    "        log_prior += dirichlet.logpdf(pi, 1e3 * np.ones(self.clusters)) + np.log(jacobian_stick_breaking(z['pi']))\n",
    "        for k in range(self.clusters):\n",
    "            matrix.append(np.log(pi[k]) + np.sum(norm.logpdf(x, mu[(k * self.D):((k + 1) * self.D)],\n",
    "                                np.full([self.D], 1./np.sqrt(tau[k]))), 1))\n",
    "        matrix  = np.vstack(matrix)\n",
    "        vector = logsumexp(matrix, axis=0)\n",
    "        log_lik = np.sum(vector)        \n",
    "        return self.scale * log_lik + log_prior    \n",
    "    \n",
    "    \n",
    "    def predict(self, z):\n",
    "        x = self.data\n",
    "        mu, tau, pi = z['mu'], softplus(z['tau']), stick_breaking(z['pi'])        \n",
    "        matrix = []                \n",
    "        for k in range(self.clusters):\n",
    "            matrix.append(np.log(pi[k]) + np.sum(norm.logpdf(x, mu[(k * self.D):((k + 1) * self.D)],\n",
    "                                 np.full([self.D], 1./np.sqrt(tau[k]))), 1))\n",
    "        matrix  = np.vstack(matrix)                \n",
    "        return np.argmax(matrix, 0)    \n",
    "    \n",
    " \n",
    "    def q_log_prob(self, means, log_sigmas, z):\n",
    "        q_w = np.sum(norm.logpdf(z, means, np.exp(log_sigmas)))        \n",
    "        return q_w\n",
    "      \n",
    "    def q_log_prob_sep(self, means, log_sigmas, z):\n",
    "        q_w = norm.logpdf(z, means, np.exp(log_sigmas))        \n",
    "        return q_w\n",
    "    \n",
    "    def sample_q(self, means, log_sigmas, d):        \n",
    "        eps = npr.randn(d)        \n",
    "        q_s = np.exp(log_sigmas) * eps + means\n",
    "        return (q_s, eps)\n",
    "        \n",
    "    def grad_params(self, dp_log_prob, eps, log_sigmas):                \n",
    "        grad_means = dp_log_prob\n",
    "        grad_log_sigmas = dp_log_prob*eps*np.exp(log_sigmas) + 1                \n",
    "        return np.concatenate([grad_means, grad_log_sigmas])\n",
    "        \n",
    "    def calc_eps(self, means, log_sigma, z):        \n",
    "        eps  = (z - means)/np.exp(log_sigma)\n",
    "        return eps            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Inference(object):      \n",
    "    def __init__(self, model, params):\n",
    "        self.model = model\n",
    "        self.params = params\n",
    "\n",
    "    def run(self, epochs, batch_size, samples, learning_rate, r, algorithm = 'SGD', optimizer = 'adam'):\n",
    "        epochs = epochs\n",
    "        batches = self.model.N/batch_size\n",
    "        batch_size = batch_size\n",
    "        samples = samples\n",
    "        learning_rate = learning_rate        \n",
    "        \n",
    "        means, unflatten = flatten(self.params['means'])\n",
    "        log_sigmas, unflatten = flatten(self.params['log_sigmas'])        \n",
    "        D =len(means)\n",
    "\n",
    "        self.F = np.zeros(epochs * batches)\n",
    "        self.time = np.zeros(epochs * batches)\n",
    "        adam = Adam(2*D)\n",
    "        f = 0\n",
    "        \n",
    "        grad_p_log_prob = grad(model.p_log_prob, argnum = 1)\n",
    "        grad_q_log_prob = grad(model.q_log_prob, argnum = 1)\n",
    "        \n",
    "        if algorithm == 'SGD':\n",
    "            for e in range(epochs):\n",
    "                for b in range(batches): \n",
    "                    start = time.clock()\n",
    "                    losses = 0.\n",
    "                    d_elbo = 0.\n",
    "                    idx = np.random.choice(np.arange(self.model.N), batch_size, replace=False)                    \n",
    "                    d_elbo = 0.\n",
    "\n",
    "                    for s in range(samples):\n",
    "                        eps = npr.randn(D)        \n",
    "                        z = np.exp(log_sigmas) * eps + means                                            \n",
    "                        p_log_prob = model.p_log_prob(idx, unflatten(z))                        \n",
    "                        dp_log_prob, _ = flatten(grad_p_log_prob(idx, unflatten(z)))\n",
    "                        g =  model.grad_params(dp_log_prob, eps, log_sigmas)                        \n",
    "                        d_elbo += g\n",
    "                        q_log_prob = model.q_log_prob(means, log_sigmas, z)                                         \n",
    "                        losses +=  (p_log_prob - q_log_prob)                    \n",
    "                    loss = losses/samples\n",
    "                    d_elbo /= samples   \n",
    "                    means_old, log_sigmas_old = means, log_sigmas\n",
    "                    means, log_sigmas = adam.update(d_elbo, np.concatenate([means, log_sigmas]), learning_rate)\n",
    "                    if np.sum(np.isnan(means)) > 0 or np.sum(np.isnan(log_sigmas)) > 0:\n",
    "                        means, log_sigmas = means_old, log_sigmas_old\n",
    "                        learning_rate = learning_rate * .1                        \n",
    "                    self.F[f] =  -loss                \n",
    "\n",
    "                    stop = time.clock()\n",
    "                    self.time[f] = stop - start\n",
    "                    f+=1\n",
    "                if e % 1 == 0:\n",
    "                    pstate = 'Epoch = ' + \"{0:0=3d}\".format(e) + ': Loss = {0:.3f}'.format(self.F[f-1])\n",
    "                    print (pstate, end = '\\r')\n",
    "                    sys.stdout.flush()    \n",
    "                    \n",
    "                            \n",
    "        if algorithm == 'iSGD':\n",
    "            n = 1.  \n",
    "            z_old = [0.] * samples\n",
    "            dp_log_prob_old = [0.] * samples\n",
    "            q_log_prob_old = [0.] * samples            \n",
    "\n",
    "            for e in range(epochs):\n",
    "                for b in range(batches): \n",
    "                    start = time.clock()\n",
    "                    losses = 0.\n",
    "                    d_elbo = 0.\n",
    "                    idx = np.random.choice(np.arange(self.model.N), batch_size, replace=False)                                        \n",
    "                    d_elbo = 0.\n",
    "                    if n > .5:\n",
    "                        for s in range(samples):\n",
    "                            eps = npr.randn(D)        \n",
    "                            z = np.exp(log_sigmas) * eps + means\n",
    "                            p_log_prob = model.p_log_prob(idx, unflatten(z))\n",
    "                            q_log_prob = model.q_log_prob_sep(means, log_sigmas, z)                                         \n",
    "                            dp_log_prob, _ = flatten(grad_p_log_prob(idx, unflatten(z)))\n",
    "                            g =  model.grad_params(dp_log_prob, eps, log_sigmas)                        \n",
    "                            d_elbo += g\n",
    "                            losses +=  (p_log_prob - np.sum(q_log_prob))\n",
    "                                        \n",
    "                            z_old[s] = z\n",
    "                            dp_log_prob_old[s] = dp_log_prob\n",
    "                            q_log_prob_old[s] = q_log_prob                            \n",
    "\n",
    "                        loss = losses/samples\n",
    "                        d_elbo /= samples                                            \n",
    "                    else:                         \n",
    "                        for s in range(samples):                            \n",
    "                            eps = (z_old[s] - means)/np.exp(log_sigmas)                            \n",
    "                            q_log_prob = model.q_log_prob_sep(means, log_sigmas, z_old[s])                                         \n",
    "                            w = np.exp(q_log_prob - q_log_prob_old[s])                            \n",
    "                            g =  model.grad_params(w * dp_log_prob_old[s], eps, log_sigmas)                        \n",
    "                            d_elbo += g                            \n",
    "                        d_elbo /= samples                                             \n",
    "                    n = npr.uniform()\n",
    "                    means_old, log_sigmas_old = means, log_sigmas\n",
    "                    means, log_sigmas = adam.update(d_elbo, np.concatenate([means, log_sigmas]), learning_rate)\n",
    "                    if np.sum(np.isnan(means)) > 0 or np.sum(np.isnan(log_sigmas)) > 0:\n",
    "                        means, log_sigmas = means_old, log_sigmas_old\n",
    "                        learning_rate = learning_rate * .9   \n",
    "                        n = 1.\n",
    "                    self.F[f] =  -loss                \n",
    "                    stop = time.clock()\n",
    "                    self.time[f] = stop - start\n",
    "                    f+=1                    \n",
    "                if e % 1 == 0:\n",
    "                    pstate = 'Epoch = ' + \"{0:0=3d}\".format(e) + ': Loss = {0:.3f}'.format(self.F[f-1])\n",
    "                    print (pstate, end = '\\r')\n",
    "                    sys.stdout.flush()  \n",
    "                                        \n",
    "                             \n",
    "        if algorithm == 'iSGDr':\n",
    "            n = 1.  \n",
    "            z_old = [0.] * samples\n",
    "            dp_log_prob_old = [0.] * samples\n",
    "            q_log_prob_old = [0.] * samples            \n",
    "\n",
    "            for e in range(epochs):\n",
    "                for b in range(batches): \n",
    "                    start = time.clock()\n",
    "                    losses = 0.\n",
    "                    d_elbo = 0.\n",
    "                    idx = np.random.choice(np.arange(self.model.N), batch_size, replace=False)                                        \n",
    "                    d_elbo = 0.\n",
    "                    if n > r:\n",
    "                        for s in range(samples):\n",
    "                            eps = npr.randn(D)        \n",
    "                            z = np.exp(log_sigmas) * eps + means\n",
    "                            p_log_prob = model.p_log_prob(idx, unflatten(z))\n",
    "                            q_log_prob = model.q_log_prob_sep(means, log_sigmas, z)                                         \n",
    "                            dp_log_prob, _ = flatten(grad_p_log_prob(idx, unflatten(z)))\n",
    "                            g =  model.grad_params(dp_log_prob, eps, log_sigmas)                        \n",
    "                            d_elbo += g\n",
    "                            losses +=  (p_log_prob - np.sum(q_log_prob))\n",
    "                                        \n",
    "                            z_old[s] = z\n",
    "                            dp_log_prob_old[s] = dp_log_prob\n",
    "                            q_log_prob_old[s] = q_log_prob                            \n",
    "\n",
    "                        loss = losses/samples\n",
    "                        d_elbo /= samples                                            \n",
    "                    else:                         \n",
    "                        for s in range(samples):                            \n",
    "                            eps = (z_old[s] - means)/np.exp(log_sigmas)                            \n",
    "                            q_log_prob = model.q_log_prob_sep(means, log_sigmas, z_old[s])                                         \n",
    "                            w = np.exp(q_log_prob - q_log_prob_old[s])                            \n",
    "                            g =  model.grad_params(w * dp_log_prob_old[s], eps, log_sigmas)                        \n",
    "                            d_elbo += g                            \n",
    "                        d_elbo /= samples                                             \n",
    "                    n = npr.uniform()\n",
    "                    means_old, log_sigmas_old = means, log_sigmas\n",
    "                    means, log_sigmas = adam.update(d_elbo, np.concatenate([means, log_sigmas]), learning_rate)\n",
    "                    if np.sum(np.isnan(means)) > 0 or np.sum(np.isnan(log_sigmas)) > 0:\n",
    "                        means, log_sigmas = means_old, log_sigmas_old\n",
    "                        learning_rate = learning_rate * .9   \n",
    "                        n = 1.\n",
    "                    self.F[f] =  -loss                \n",
    "                    stop = time.clock()\n",
    "                    self.time[f] = stop - start\n",
    "                    f+=1                               \n",
    "                if e % 1 == 0:\n",
    "                    pstate = 'Epoch = ' + \"{0:0=3d}\".format(e) + ': Loss = {0:.3f}'.format(self.F[f-1])\n",
    "                    print (pstate, end = '\\r')\n",
    "                    sys.stdout.flush()  \n",
    "                                        \n",
    "        self.params = {'means': unflatten(means), 'log_sigmas': unflatten(log_sigmas)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "N = n_samples = 5000\n",
    "epochs = 10\n",
    "samples = 1\n",
    "learning_rate = 0.1\n",
    "K = clusters = 10\n",
    "seed = 222\n",
    "batch_size = 2000\n",
    "D = 2\n",
    "\n",
    "\n",
    "npr.seed(seed)\n",
    "c =  5 * npr.randn(K * D).reshape([K,D])\n",
    "m = np.tile(c, (N/K,1))\n",
    "X =  npr.randn(N * D).reshape([N,D]) + m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 199: Loss = 26471.200\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff6f9b097d0>]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdkAAAFJCAYAAADXIVdBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff6fb067f90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "epochs = 200\n",
    "samples = 1\n",
    "learning_rate = .1\n",
    "seed = 111\n",
    "data = X\n",
    "    \n",
    "N = data.shape[0]\n",
    "D = data.shape[1]\n",
    "model = GMM(data, K, N/batch_size)\n",
    "\n",
    "        \n",
    "#f, (ax1, ax2, ax3) = plt.subplots(3,1,figsize=(30,30))\n",
    "# npr.seed(seed)    \n",
    "# params = {}\n",
    "# params['means'] = {'mu': np.ones(K*D), 'tau': npr.randn(K), 'pi':  npr.randn(K-1)}\n",
    "# params['log_sigmas'] = {'mu': npr.randn(K*D), 'tau': npr.randn(K), 'pi':  npr.randn(K-1)}\n",
    "# inference = Inference(model, params)\n",
    "# inference.run(epochs, batch_size, samples, learning_rate, 'SGD')\n",
    "# ax1.plot(np.cumsum(inference.time), -inference.F, color = color_iter.next())\n",
    "# p = model.predict(inference.params['means'])\n",
    "# means_ = inference.params['means']['mu'].reshape([clusters, D])\n",
    "# covariances_ = 1/np.sqrt(np.exp(inference.params['means']['tau']))\n",
    "# pi_ = stick_breaking(inference.params['means']['pi'])\n",
    "# plot_results(ax2, X, p, means_, covariances_, 0, 'Bayesian GMM')\n",
    "\n",
    "final = []\n",
    "rv = [.1, .3, .5, .7, .9]#, .6, .7, .9]\n",
    "for r in rv:\n",
    "    npr.seed(seed)    \n",
    "    params = {}\n",
    "    params['means'] = {'mu': npr.randn(K*D), 'tau': npr.randn(K), 'pi':  npr.randn(K-1)}\n",
    "    params['log_sigmas'] = {'mu': npr.randn(K*D), 'tau': npr.randn(K), 'pi':  npr.randn(K-1)}\n",
    "    inference = Inference(model, params)\n",
    "    inference.run(epochs, batch_size, samples, learning_rate, r, 'iSGDr')\n",
    "    f = -np.mean(inference.F.reshape([N/batch_size, -1]),1)\n",
    "    t = np.cumsum(np.sum(inference.time.reshape([N/batch_size, -1]),1))\n",
    "    final.append(np.sum(inference.time[inference.F > 25500]))\n",
    "    #plt.plot(t, f)\n",
    "    #final.append(f)    \n",
    "    #plt.plot(t,f)\n",
    "    #plt.plot(np.cumsum(inference.time), -inference.F, color = color_iter.next())\n",
    "    #times.append(np.sum(inference.time[inference.F > 26000]))\n",
    "    #times.append(np.sum(inference.time))\n",
    "plt.plot(rv, final, marker = 'o')\n",
    "#    p = model.predict(inference.params['means'])\n",
    "#     means_ = inference.params['means']['mu'].reshape([clusters, D])\n",
    "#     covariances_ = 1/np.sqrt(np.exp(inference.params['means']['tau']))\n",
    "#     pi_ = stick_breaking(inference.params['means']['pi'])\n",
    "#     plot_results(ax3, X, p, means_, covariances_, 0, 'Bayesian GMM')\n",
    "\n",
    "# npr.seed(seed)    \n",
    "# params = {}\n",
    "# params['means'] = {'mu': np.ones(K*D), 'tau': npr.randn(K), 'pi':  npr.randn(K-1)}\n",
    "# params['log_sigmas'] = {'mu': npr.randn(K*D), 'tau': npr.randn(K), 'pi':  npr.randn(K-1)}\n",
    "# inference = Inference(model, params)\n",
    "# inference.run(epochs, batch_size, samples, learning_rate, 'iSRA')\n",
    "# ax1.plot(np.cumsum(inference.time), -inference.F, color = colors[1])\n",
    "# p = model.predict(inference.params['means'])\n",
    "# means_ = inference.params['means']['mu'].reshape([clusters, D])\n",
    "# covariances_ = 1/np.sqrt(np.exp(inference.params['means']['tau']))\n",
    "# pi_ = stick_breaking(inference.params['means']['pi'])\n",
    "# plot_results(ax3, X, p, means_, covariances_, 0, 'Bayesian GMM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 43987.52949358  35471.66770409  33076.80679795  31362.63011021\n",
      "  30597.06230822  30482.46944996  29594.91865188  29379.46897037\n",
      "  29277.86418124  28925.22907205  29077.71288878  28906.61990234\n",
      "  28569.56900671  28642.53996242  28788.24335187  28606.61516855\n",
      "  28080.46365691  28189.00723168  28050.19216749  27874.85406717\n",
      "  27984.95746483  27868.52727403  27975.62389623  28116.46216528\n",
      "  27743.17093159  27871.63014603  27741.94206099  27394.03343568\n",
      "  27394.84972316  26656.22092354  26511.85861129  26636.36618563\n",
      "  26391.49413335  26510.31086116  26604.41090273  26371.22813877\n",
      "  26014.94156406  26308.62925207  26245.72568335  26087.76467626\n",
      "  25824.25800756  25844.22444234  25931.69132582  26096.00596448\n",
      "  25562.64101319  25529.70956758  25188.95336993  25474.31380179\n",
      "  25394.87868496  25593.99528239]\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(inference.F.reshape([N/batch_size, -1]),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
