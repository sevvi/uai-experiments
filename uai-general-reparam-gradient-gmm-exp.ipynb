{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import argparse, gzip, cPickle, sys, time, itertools\n",
    "\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "import autograd.scipy.stats.norm as norm\n",
    "import autograd.scipy.stats.dirichlet as dirichlet\n",
    "from autograd.scipy.misc import logsumexp\n",
    "from autograd.util import flatten_func, flatten\n",
    "from autograd import grad, primitive\n",
    "from autograd.numpy.numpy_grads import unbroadcast\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import mixture\n",
    "\n",
    "from  autograd.scipy.special import gammaln, digamma, gamma\n",
    "from scipy import linalg\n",
    "from scipy import stats, integrate\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.animation as animation\n",
    "import pandas as pd\n",
    "import six\n",
    "\n",
    "\n",
    "color_names = [\"maroon\",               \n",
    "               \"gold\",\n",
    "               \"royal blue\"]\n",
    "colors = sns.xkcd_palette(color_names)\n",
    "sns.set_style(\"white\")\n",
    "color_iter = itertools.cycle(colors)\n",
    "\n",
    "def plot_results(ax, X, Y, means, covariances, index, title):    \n",
    "    for i, (mean, covar, color) in enumerate(zip(\n",
    "             means, covariances, color_iter)):\n",
    "        v, w = linalg.eigh(np.diag(np.full([2], covar)))\n",
    "        v = 2. * np.sqrt(2.) * np.sqrt(v)\n",
    "        u = w[0] / linalg.norm(w[0])      \n",
    "\n",
    "        if not np.any(Y == i):\n",
    "            continue\n",
    "        ax.scatter(X[Y == i, 0], X[Y == i, 1], 2., color=color)\n",
    "\n",
    "        angle = np.arctan(u[1] / u[0])\n",
    "        angle = 180. * angle / np.pi  # convert to degrees\n",
    "        ell = mpl.patches.Ellipse(mean, v[0], v[1], 180. + angle, color=color)\n",
    "        #ell.set_clip_box(splot.bbox)\n",
    "        ell.set_alpha(0.5)\n",
    "        ax.add_artist(ell)\n",
    "        \n",
    "\n",
    "class Adam(object):\n",
    "    def __init__(self, dparam, b1=0.9, b2=0.999, eps=1,#10**-8,\n",
    "                         decay_rate = 0.9, decay_steps = 100):                    \n",
    "        self.b1 = b1;\n",
    "        self.b2 = b2;\n",
    "        self.eps = eps        \n",
    "        self.m = np.zeros(dparam)\n",
    "        self.v = np.zeros(dparam)\n",
    "        self.i = 0\n",
    "        self.decay_rate = decay_rate\n",
    "        self.decay_steps = decay_steps\n",
    "    \n",
    "    def update(self, gradients, params, learning_rate = 0.1):        \n",
    "        self.i = self.i+1\n",
    "        step_size = learning_rate * self.decay_rate**(self.i/self.decay_steps)        \n",
    "        self.m = (1 - self.b1) * gradients + self.b1 * self.m\n",
    "        self.v = (1 - self.b2) * (gradients**2) + self.b2 * self.v\n",
    "        mhat = self.m / (1 - self.b1**(self.i))\n",
    "        vhat = self.v / (1 - self.b2**(self.i))                        \n",
    "        params = params + step_size*mhat/(np.sqrt(vhat) + self.eps)        \n",
    "        return np.split(params,2)\n",
    "\n",
    "    \n",
    "@primitive\n",
    "def softplus(x):\n",
    "    return np.log(1. + np.exp(x))\n",
    "\n",
    "softplus.defvjp(lambda g, ans, vs, gvs, x: unbroadcast(vs, gvs, g * 1./(1. + np.exp(-x))))\n",
    "\n",
    "def jacobian_softplus(x):\n",
    "    return 1./(1. + np.exp(-x))\n",
    "\n",
    "@primitive\n",
    "def gamma_logpdf(x, alpha = 1., beta = 1.):\n",
    "    return  (alpha*np.log(beta) + (alpha - 1)*np.log(x) - x*beta - gammaln(alpha))\n",
    "\n",
    "gamma_logpdf.defvjp(lambda g, ans, vs, gvs, x, alpha=1.0, beta=1.0: unbroadcast(vs, gvs, g * ((alpha-1)/x - beta)))\n",
    "gamma_logpdf.defvjp(lambda g, ans, vs, gvs, x, alpha=1.0, beta=1.0: unbroadcast(vs, gvs,  g * (np.log(beta) + np.log(x) - digamma(alpha))), argnum=1)\n",
    "gamma_logpdf.defvjp(lambda g, ans, vs, gvs, x, alpha=1.0, beta=1.0: unbroadcast(vs, gvs,  g * (alpha/beta - x)), argnum=2)\n",
    "\n",
    "\n",
    "cumprod = primitive(np.cumprod)\n",
    "\n",
    "def grad_np_cumprod(g, ans, vs, gvs, x, axis=None):\n",
    "    fx = np.cumprod(x, axis=None)\n",
    "    return np.cumsum((fx * g)[::-1])[::-1].reshape(x.shape) / x\n",
    "\n",
    "cumprod.defvjp(grad_np_cumprod)\n",
    "\n",
    "def invlogit(x, eps=sys.float_info.epsilon):\n",
    "    return (1 - 2 * eps) / (1 + np.exp(-x)) + eps\n",
    "\n",
    "def logit(p):\n",
    "    return np.log(p / (1 - p))\n",
    "\n",
    "def stick_breaking(y_):\n",
    "    y = y_.T\n",
    "    Km1 = y.shape[0]\n",
    "    k = np.arange(Km1)[(slice(None), ) + (None, ) * (y.ndim - 1)]\n",
    "    eq_share = logit(1. / (Km1 + 1 - k).astype(str(y_.dtype)))\n",
    "    z = invlogit(y + eq_share, 1e-3)\n",
    "    yl = np.concatenate([z, np.ones(y[:1].shape)])\n",
    "    yu = np.concatenate([np.ones(y[:1].shape), 1 - z])\n",
    "    S = cumprod(yu, 0)\n",
    "    x = S * yl\n",
    "    return x.T\n",
    "\n",
    "def jacobian_stick_breaking(y_):\n",
    "    y = y_.T\n",
    "    Km1 = y.shape[0]\n",
    "    k = np.arange(Km1)[(slice(None), ) + (None, ) * (y.ndim - 1)]\n",
    "    eq_share = logit(1. / (Km1 + 1 - k).astype(str(y_.dtype)))\n",
    "    yl = y + eq_share\n",
    "    yu = np.concatenate([np.ones(y[:1].shape), 1 - invlogit(yl, 1e-3)])\n",
    "    S = cumprod(yu, 0)\n",
    "    return -np.sum(np.log(S[:-1]) - np.log1p(np.exp(yl)) - np.log1p(np.exp(-yl)), 0).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class GMM(object):\n",
    "    def __init__(self, data, clusters, scale):\n",
    "        self.data = data     \n",
    "        self.clusters = clusters\n",
    "        self.scale = scale\n",
    "        self.N = data.shape[0]\n",
    "        self.D = data.shape[1]        \n",
    "    \n",
    "    def p_log_prob(self, idx, z):\n",
    "        x = self.data[idx]        \n",
    "        mu, tau, pi = z['mu'], softplus(z['tau']), stick_breaking(z['pi'])\n",
    "        matrix = []  \n",
    "        log_prior = 0.\n",
    "        log_prior += np.sum(gamma_logpdf(tau, 1e-5, 1e-5) + np.log(jacobian_softplus(z['tau'])))        \n",
    "        log_prior += np.sum(norm.logpdf(mu, 0, 1.))\n",
    "        log_prior += dirichlet.logpdf(pi, 1e3 * np.ones(self.clusters)) + np.log(jacobian_stick_breaking(z['pi']))\n",
    "        for k in range(self.clusters):\n",
    "            matrix.append(np.log(pi[k]) + np.sum(norm.logpdf(x, mu[(k * self.D):((k + 1) * self.D)],\n",
    "                                np.full([self.D], 1./np.sqrt(tau[k]))), 1))\n",
    "        matrix  = np.vstack(matrix)\n",
    "        vector = logsumexp(matrix, axis=0)\n",
    "        log_lik = np.sum(vector)        \n",
    "        return self.scale * log_lik + log_prior    \n",
    "    \n",
    "    \n",
    "    def predict(self, z):\n",
    "        x = self.data\n",
    "        mu, tau, pi = z['mu'], softplus(z['tau']), stick_breaking(z['pi'])        \n",
    "        matrix = []                \n",
    "        for k in range(self.clusters):\n",
    "            matrix.append(np.log(pi[k]) + np.sum(norm.logpdf(x, mu[(k * self.D):((k + 1) * self.D)],\n",
    "                                 np.full([self.D], 1./np.sqrt(tau[k]))), 1))\n",
    "        matrix  = np.vstack(matrix)                \n",
    "        return np.argmax(matrix, 0)    \n",
    "    \n",
    " \n",
    "    def q_log_prob(self, means, log_sigmas, z):\n",
    "        q_w = np.sum(norm.logpdf(z, means, np.exp(log_sigmas)))        \n",
    "        return q_w\n",
    "      \n",
    "    def q_log_prob_sep(self, means, log_sigmas, z):\n",
    "        q_w = norm.logpdf(z, means, np.exp(log_sigmas))        \n",
    "        return q_w\n",
    "    \n",
    "    def sample_q(self, means, log_sigmas, d):        \n",
    "        eps = npr.randn(d)        \n",
    "        q_s = np.exp(log_sigmas) * eps + means\n",
    "        return (q_s, eps)\n",
    "        \n",
    "    def grad_params(self, dp_log_prob, eps, log_sigmas):                \n",
    "        grad_means = dp_log_prob\n",
    "        grad_log_sigmas = dp_log_prob*eps*np.exp(log_sigmas) + 1                \n",
    "        return np.concatenate([grad_means, grad_log_sigmas])\n",
    "        \n",
    "    def calc_eps(self, means, log_sigma, z):        \n",
    "        eps  = (z - means)/np.exp(log_sigma)\n",
    "        return eps            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Inference(object):      \n",
    "    def __init__(self, model, params):\n",
    "        self.model = model\n",
    "        self.params = params\n",
    "\n",
    "    def run(self, epochs, batch_size, samples, learning_rate, algorithm = 'SGD', optimizer = 'adam'):\n",
    "        epochs = epochs\n",
    "        batches = self.model.N/batch_size\n",
    "        batch_size = batch_size\n",
    "        samples = samples\n",
    "        learning_rate = learning_rate        \n",
    "        \n",
    "        means, unflatten = flatten(self.params['means'])\n",
    "        log_sigmas, unflatten = flatten(self.params['log_sigmas'])        \n",
    "        D =len(means)\n",
    "\n",
    "        self.F = np.zeros(epochs * batches)\n",
    "        self.time = np.zeros(epochs * batches)\n",
    "        adam = Adam(2*D)\n",
    "        f = 0\n",
    "        \n",
    "        grad_p_log_prob = grad(model.p_log_prob, argnum = 1)\n",
    "        grad_q_log_prob = grad(model.q_log_prob, argnum = 1)\n",
    "        \n",
    "        if algorithm == 'SGD':\n",
    "            for e in range(epochs):\n",
    "                for b in range(batches): \n",
    "                    start = time.clock()\n",
    "                    losses = 0.\n",
    "                    d_elbo = 0.\n",
    "                    idx = np.random.choice(np.arange(self.model.N), batch_size, replace=False)                    \n",
    "                    d_elbo = 0.\n",
    "\n",
    "                    for s in range(samples):\n",
    "                        eps = npr.randn(D)        \n",
    "                        z = np.exp(log_sigmas) * eps + means                                            \n",
    "                        p_log_prob = model.p_log_prob(idx, unflatten(z))                        \n",
    "                        dp_log_prob, _ = flatten(grad_p_log_prob(idx, unflatten(z)))\n",
    "                        g =  model.grad_params(dp_log_prob, eps, log_sigmas)                        \n",
    "                        d_elbo += g\n",
    "                        q_log_prob = model.q_log_prob(means, log_sigmas, z)                                         \n",
    "                        losses +=  (p_log_prob - q_log_prob)                    \n",
    "                    loss = losses/samples\n",
    "                    d_elbo /= samples   \n",
    "                    means_old, log_sigmas_old = means, log_sigmas\n",
    "                    means, log_sigmas = adam.update(d_elbo, np.concatenate([means, log_sigmas]), learning_rate)\n",
    "                    if np.sum(np.isnan(means)) > 0 or np.sum(np.isnan(log_sigmas)) > 0:\n",
    "                        means, log_sigmas = means_old, log_sigmas_old\n",
    "                        learning_rate = learning_rate * .1                        \n",
    "                    self.F[f] =  -loss                \n",
    "\n",
    "                    stop = time.clock()\n",
    "                    self.time[f] = stop - start\n",
    "                    f+=1\n",
    "                if e % 1 == 0:\n",
    "                    pstate = 'Epoch = ' + \"{0:0=3d}\".format(e) + ': Loss = {0:.3f}'.format(self.F[f-1])\n",
    "                    print (pstate, end = '\\r')\n",
    "                    sys.stdout.flush()    \n",
    "                    \n",
    "                            \n",
    "        if algorithm == 'iSGD':\n",
    "            n = 1.  \n",
    "            z_old = [0.] * samples\n",
    "            dp_log_prob_old = [0.] * samples\n",
    "            q_log_prob_old = [0.] * samples            \n",
    "\n",
    "            for e in range(epochs):\n",
    "                for b in range(batches): \n",
    "                    start = time.clock()\n",
    "                    losses = 0.\n",
    "                    d_elbo = 0.\n",
    "                    idx = np.random.choice(np.arange(self.model.N), batch_size, replace=False)                                        \n",
    "                    d_elbo = 0.\n",
    "                    if n > .9:\n",
    "                        for s in range(samples):\n",
    "                            eps = npr.randn(D)        \n",
    "                            z = np.exp(log_sigmas) * eps + means\n",
    "                            p_log_prob = model.p_log_prob(idx, unflatten(z))\n",
    "                            q_log_prob = model.q_log_prob_sep(means, log_sigmas, z)                                         \n",
    "                            dp_log_prob, _ = flatten(grad_p_log_prob(idx, unflatten(z)))\n",
    "                            g =  model.grad_params(dp_log_prob, eps, log_sigmas)                        \n",
    "                            d_elbo += g\n",
    "                            losses +=  (p_log_prob - np.sum(q_log_prob))\n",
    "                                        \n",
    "                            z_old[s] = z\n",
    "                            dp_log_prob_old[s] = dp_log_prob\n",
    "                            q_log_prob_old[s] = q_log_prob                            \n",
    "\n",
    "                        loss = losses/samples\n",
    "                        d_elbo /= samples                                            \n",
    "                    else:                         \n",
    "                        for s in range(samples):                            \n",
    "                            eps = (z_old[s] - means)/np.exp(log_sigmas)                            \n",
    "                            q_log_prob = model.q_log_prob_sep(means, log_sigmas, z_old[s])                                         \n",
    "                            w = np.exp(q_log_prob - q_log_prob_old[s])                            \n",
    "                            g =  model.grad_params(w * dp_log_prob_old[s], eps, log_sigmas)                        \n",
    "                            d_elbo += g                            \n",
    "                        d_elbo /= samples                                             \n",
    "                    n = npr.uniform()\n",
    "                    means_old, log_sigmas_old = means, log_sigmas\n",
    "                    means, log_sigmas = adam.update(d_elbo, np.concatenate([means, log_sigmas]), learning_rate)\n",
    "                    if np.sum(np.isnan(means)) > 0 or np.sum(np.isnan(log_sigmas)) > 0:\n",
    "                        means, log_sigmas = means_old, log_sigmas_old\n",
    "                        learning_rate = learning_rate * .9   \n",
    "                        n = 1.\n",
    "                    self.F[f] =  -loss                \n",
    "                    stop = time.clock()\n",
    "                    self.time[f] = stop - start\n",
    "                    f+=1                    \n",
    "                if e % 1 == 0:\n",
    "                    pstate = 'Epoch = ' + \"{0:0=3d}\".format(e) + ': Loss = {0:.3f}'.format(self.F[f-1])\n",
    "                    print (pstate, end = '\\r')\n",
    "                    sys.stdout.flush()  \n",
    "                                        \n",
    "        if algorithm == 'iSRA':\n",
    "            n = 1.  \n",
    "            alpha = .8\n",
    "            z_old = [0.] * samples\n",
    "            dp_log_prob_old = [0.] * samples\n",
    "            q_log_prob_old = [0.] * samples            \n",
    "            d_elbo_avg = 0.\n",
    "            for e in range(epochs):\n",
    "                for b in range(batches): \n",
    "                    start = time.clock()\n",
    "                    losses = 0.\n",
    "                    d_elbo = 0.\n",
    "                    idx = np.random.choice(np.arange(self.model.N), batch_size, replace=False)                                        \n",
    "                    d_elbo = 0.\n",
    "                    if n > .7:\n",
    "                        for s in range(samples):\n",
    "                            eps = npr.randn(D)        \n",
    "                            z = np.exp(log_sigmas) * eps + means\n",
    "                            p_log_prob = model.p_log_prob(idx, unflatten(z))\n",
    "                            q_log_prob = model.q_log_prob_sep(means, log_sigmas, z)                                         \n",
    "                            dp_log_prob, _ = flatten(grad_p_log_prob(idx, unflatten(z)))\n",
    "                            g =  model.grad_params(dp_log_prob, eps, log_sigmas)                        \n",
    "                            d_elbo += g\n",
    "                            losses +=  (p_log_prob - np.sum(q_log_prob))\n",
    "                                        \n",
    "                            z_old[s] = z\n",
    "                            dp_log_prob_old[s] = dp_log_prob\n",
    "                            q_log_prob_old[s] = q_log_prob                            \n",
    "\n",
    "                        loss = losses/samples\n",
    "                        d_elbo /= samples                                                                    \n",
    "                        d_elbo_avg = alpha*d_elbo + (1 - alpha)*d_elbo_avg\n",
    "                    else:                         \n",
    "                        for s in range(samples):                            \n",
    "                            eps = (z_old[s] - means)/np.exp(log_sigmas)                            \n",
    "                            q_log_prob = model.q_log_prob_sep(means, log_sigmas, z_old[s])                                         \n",
    "                            w = np.exp(q_log_prob - q_log_prob_old[s])                            \n",
    "                            g =  model.grad_params(w * dp_log_prob_old[s], eps, log_sigmas)                        \n",
    "                            d_elbo += g                            \n",
    "                        d_elbo /= samples                                             \n",
    "                        d_elbo_avg = alpha*d_elbo + (1 - alpha)*d_elbo_avg\n",
    "                    n = npr.uniform()                    \n",
    "                    means_old, log_sigmas_old = means, log_sigmas\n",
    "                    means, log_sigmas = adam.update(d_elbo_avg, np.concatenate([means, log_sigmas]), learning_rate)\n",
    "                    if np.sum(np.isnan(means)) > 0 or np.sum(np.isnan(log_sigmas)) > 0:\n",
    "                        means, log_sigmas = means_old, log_sigmas_old\n",
    "                        learning_rate = learning_rate * .9\n",
    "                        n = 1.                    \n",
    "                    self.F[f] =  -loss                \n",
    "                    stop = time.clock()\n",
    "                    self.time[f] = stop - start\n",
    "                    f+=1                    \n",
    "                if e % 1 == 0:\n",
    "                    pstate = 'Epoch = ' + \"{0:0=3d}\".format(e) + ': Loss = {0:.3f}'.format(self.F[f-1])\n",
    "                    print (pstate, end = '\\r')\n",
    "                    sys.stdout.flush()  \n",
    "                    \n",
    "        self.params = {'means': unflatten(means), 'log_sigmas': unflatten(log_sigmas)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "N = n_samples = 10000\n",
    "epochs = 10\n",
    "samples = 1\n",
    "learning_rate = 0.1\n",
    "K = clusters = 10\n",
    "seed = 222\n",
    "batch_size = 2000\n",
    "D = 2\n",
    "\n",
    "\n",
    "npr.seed(seed)\n",
    "c =  5 * npr.randn(K * D).reshape([K,D])\n",
    "m = np.tile(c, (N/K,1))\n",
    "X =  npr.randn(N * D).reshape([N,D]) + m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 019: Loss = 50508.611\r"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAS4AAADTCAYAAAA/O/4wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCSFMzv8Ddg9A5hmt42QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f57d6a27950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "epochs = 20\n",
    "samples = 1\n",
    "learning_rate = .1\n",
    "seed = 111\n",
    "data = X\n",
    "    \n",
    "N = data.shape[0]\n",
    "D = data.shape[1]\n",
    "model = GMM(data, K, N/batch_size)\n",
    "\n",
    "        \n",
    "#f, (ax1, ax2, ax3) = plt.subplots(3,1,figsize=(30,30))\n",
    "\n",
    "\n",
    "npr.seed(seed)    \n",
    "params = {}\n",
    "params['means'] = {'mu': np.ones(K*D), 'tau': npr.randn(K), 'pi':  npr.randn(K-1)}\n",
    "params['log_sigmas'] = {'mu': npr.randn(K*D), 'tau': npr.randn(K), 'pi':  npr.randn(K-1)}\n",
    "inference = Inference(model, params)\n",
    "inference.run(epochs, batch_size, samples, learning_rate, 'SGD')\n",
    "plt.plot(np.cumsum(inference.time), -inference.F, color = colors[1],  label = \"SGD\")\n",
    "# p = model.predict(inference.params['means'])\n",
    "# means_ = inference.params['means']['mu'].reshape([clusters, D])\n",
    "# covariances_ = 1/np.sqrt(np.exp(inference.params['means']['tau']))\n",
    "# pi_ = stick_breaking(inference.params['means']['pi'])\n",
    "# plot_results(ax2, X, p, means_, covariances_, 0, 'Bayesian GMM')\n",
    "\n",
    "\n",
    "npr.seed(seed)    \n",
    "params = {}\n",
    "params['means'] = {'mu': np.ones(K*D), 'tau': npr.randn(K), 'pi':  npr.randn(K-1)}\n",
    "params['log_sigmas'] = {'mu': npr.randn(K*D), 'tau': npr.randn(K), 'pi':  npr.randn(K-1)}\n",
    "inference = Inference(model, params)\n",
    "inference.run(epochs, batch_size, samples, learning_rate, 'iSGD')\n",
    "plt.plot(np.cumsum(inference.time), -inference.F, color = colors[0], label = \"I-SGD\")\n",
    "# p = model.predict(inference.params['means'])\n",
    "# means_ = inference.params['means']['mu'].reshape([clusters, D])\n",
    "# covariances_ = 1/np.sqrt(np.exp(inference.params['means']['tau']))\n",
    "# pi_ = stick_breaking(inference.params['means']['pi'])\n",
    "# plot_results(ax3, X, p, means_, covariances_, 0, 'Bayesian GMM')\n",
    "\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(4,3)\n",
    "plt.ylabel('ELBO', fontsize = 15)\n",
    "plt.xlabel('CPU time', fontsize = 15)\n",
    "plt.xticks(fontsize = 10)\n",
    "plt.yticks(fontsize = 10)\n",
    "plt.legend(loc = 4, fontsize = 15)\n",
    "plt.savefig('/home/sakaya/MUPI/papers/uai17importance/gmm.png', dpi=300, bbox_inches= 'tight')\n",
    "\n",
    "# npr.seed(seed)    \n",
    "# params = {}\n",
    "# params['means'] = {'mu': np.ones(K*D), 'tau': npr.randn(K), 'pi':  npr.randn(K-1)}\n",
    "# params['log_sigmas'] = {'mu': npr.randn(K*D), 'tau': npr.randn(K), 'pi':  npr.randn(K-1)}\n",
    "# inference = Inference(model, params)\n",
    "# inference.run(epochs, batch_size, samples, learning_rate, 'iSRA')\n",
    "# ax1.plot(np.cumsum(inference.time), -inference.F, color = colors[1])\n",
    "# p = model.predict(inference.params['means'])\n",
    "# means_ = inference.params['means']['mu'].reshape([clusters, D])\n",
    "# covariances_ = 1/np.sqrt(np.exp(inference.params['means']['tau']))\n",
    "# pi_ = stick_breaking(inference.params['means']['pi'])\n",
    "# plot_results(ax3, X, p, means_, covariances_, 0, 'Bayesian GMM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
