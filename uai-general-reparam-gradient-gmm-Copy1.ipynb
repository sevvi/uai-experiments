{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import argparse, gzip, cPickle, sys, time, itertools\n",
    "\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "import autograd.scipy.stats.norm as norm\n",
    "import autograd.scipy.stats.dirichlet as dirichlet\n",
    "from autograd.scipy.misc import logsumexp\n",
    "from autograd.util import flatten_func, flatten\n",
    "from autograd import grad, primitive\n",
    "from autograd.numpy.numpy_grads import unbroadcast\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import mixture\n",
    "\n",
    "from  autograd.scipy.special import gammaln, digamma, gamma\n",
    "from scipy import linalg\n",
    "from scipy import stats, integrate\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.animation as animation\n",
    "import pandas as pd\n",
    "import six\n",
    "\n",
    "color_names =  [\"windows blue\",\n",
    "               \"red\",\n",
    "               \"gold\",\n",
    "               \"grass green\",\n",
    "               \"orange\", \"yellow\", \"cornflower\", \"dark red\", \"dark blue\", \"brown\"]\n",
    "colors = sns.xkcd_palette(color_names)\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "color_iter = itertools.cycle(colors)\n",
    "\n",
    "def plot_results(ax, X, Y, means, covariances, index, title):\n",
    "    #splot = ax.subplot(1, 1, 1 + index)\n",
    "    for i, (mean, covar, color) in enumerate(zip(\n",
    "             means, covariances, color_iter)):\n",
    "        v, w = linalg.eigh(np.diag(np.full([2], covar)))\n",
    "        v = 2. * np.sqrt(2.) * np.sqrt(v)\n",
    "        u = w[0] / linalg.norm(w[0])      \n",
    "\n",
    "        if not np.any(Y == i):\n",
    "            continue\n",
    "        ax.scatter(X[Y == i, 0], X[Y == i, 1], 2., color=color)\n",
    "\n",
    "        angle = np.arctan(u[1] / u[0])\n",
    "        angle = 180. * angle / np.pi  # convert to degrees\n",
    "        ell = mpl.patches.Ellipse(mean, v[0], v[1], 180. + angle, color=color)\n",
    "        #ell.set_clip_box(splot.bbox)\n",
    "        ell.set_alpha(0.5)\n",
    "        ax.add_artist(ell)\n",
    "        \n",
    "\n",
    "class Adam(object):\n",
    "    def __init__(self, dparam, b1=0.9, b2=0.999, eps=1,#10**-8,\n",
    "                         decay_rate = 0.9, decay_steps = 100):                    \n",
    "        self.b1 = b1;\n",
    "        self.b2 = b2;\n",
    "        self.eps = eps        \n",
    "        self.m = np.zeros(dparam)\n",
    "        self.v = np.zeros(dparam)\n",
    "        self.i = 0\n",
    "        self.decay_rate = decay_rate\n",
    "        self.decay_steps = decay_steps\n",
    "    \n",
    "    def update(self, gradients, params, learning_rate = 0.1):        \n",
    "        self.i = self.i+1\n",
    "        step_size = learning_rate * self.decay_rate**(self.i/self.decay_steps)        \n",
    "        self.m = (1 - self.b1) * gradients + self.b1 * self.m\n",
    "        self.v = (1 - self.b2) * (gradients**2) + self.b2 * self.v\n",
    "        mhat = self.m / (1 - self.b1**(self.i))\n",
    "        vhat = self.v / (1 - self.b2**(self.i))                        \n",
    "        params = params + step_size*mhat/(np.sqrt(vhat) + self.eps)        \n",
    "        return np.split(params,2)\n",
    "\n",
    "    \n",
    "@primitive\n",
    "def softplus(x):\n",
    "    return np.log(1. + np.exp(x))\n",
    "\n",
    "softplus.defvjp(lambda g, ans, vs, gvs, x: unbroadcast(vs, gvs, g * 1./(1. + np.exp(-x))))\n",
    "\n",
    "def jacobian_softplus(x):\n",
    "    return 1./(1. + np.exp(-x))\n",
    "\n",
    "@primitive\n",
    "def gamma_logpdf(x, alpha = 1., beta = 1.):\n",
    "    return  (alpha*np.log(beta) + (alpha - 1)*np.log(x) - x*beta - gammaln(alpha))\n",
    "\n",
    "gamma_logpdf.defvjp(lambda g, ans, vs, gvs, x, alpha=1.0, beta=1.0: unbroadcast(vs, gvs, g * ((alpha-1)/x - beta)))\n",
    "gamma_logpdf.defvjp(lambda g, ans, vs, gvs, x, alpha=1.0, beta=1.0: unbroadcast(vs, gvs,  g * (np.log(beta) + np.log(x) - digamma(alpha))), argnum=1)\n",
    "gamma_logpdf.defvjp(lambda g, ans, vs, gvs, x, alpha=1.0, beta=1.0: unbroadcast(vs, gvs,  g * (alpha/beta - x)), argnum=2)\n",
    "\n",
    "\n",
    "cumprod = primitive(np.cumprod)\n",
    "\n",
    "def grad_np_cumprod(g, ans, vs, gvs, x, axis=None):\n",
    "    fx = np.cumprod(x, axis=None)\n",
    "    return np.cumsum((fx * g)[::-1])[::-1].reshape(x.shape) / x\n",
    "\n",
    "cumprod.defvjp(grad_np_cumprod)\n",
    "\n",
    "def invlogit(x, eps=sys.float_info.epsilon):\n",
    "    return (1 - 2 * eps) / (1 + np.exp(-x)) + eps\n",
    "\n",
    "def logit(p):\n",
    "    return np.log(p / (1 - p))\n",
    "\n",
    "def stick_breaking(y_):\n",
    "    y = y_.T\n",
    "    Km1 = y.shape[0]\n",
    "    k = np.arange(Km1)[(slice(None), ) + (None, ) * (y.ndim - 1)]\n",
    "    eq_share = logit(1. / (Km1 + 1 - k).astype(str(y_.dtype)))\n",
    "    z = invlogit(y + eq_share, 1e-3)\n",
    "    yl = np.concatenate([z, np.ones(y[:1].shape)])\n",
    "    yu = np.concatenate([np.ones(y[:1].shape), 1 - z])\n",
    "    S = cumprod(yu, 0)\n",
    "    x = S * yl\n",
    "    return x.T\n",
    "\n",
    "def jacobian_stick_breaking(y_):\n",
    "    y = y_.T\n",
    "    Km1 = y.shape[0]\n",
    "    k = np.arange(Km1)[(slice(None), ) + (None, ) * (y.ndim - 1)]\n",
    "    eq_share = logit(1. / (Km1 + 1 - k).astype(str(y_.dtype)))\n",
    "    yl = y + eq_share\n",
    "    yu = np.concatenate([np.ones(y[:1].shape), 1 - invlogit(yl, 1e-3)])\n",
    "    S = cumprod(yu, 0)\n",
    "    return -np.sum(np.log(S[:-1]) - np.log1p(np.exp(yl)) - np.log1p(np.exp(-yl)), 0).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class GMM(object):\n",
    "    def __init__(self, data, clusters, scale):\n",
    "        self.data = data     \n",
    "        self.clusters = clusters\n",
    "        self.scale = scale\n",
    "        self.N = data.shape[0]\n",
    "        self.D = data.shape[1]        \n",
    "    \n",
    "    def p_log_prob(self, idx, z):\n",
    "        x = self.data[idx]        \n",
    "        mu, tau, pi = z['mu'], softplus(z['tau']), stick_breaking(z['pi'])\n",
    "        matrix = []  \n",
    "        log_prior = 0.\n",
    "        log_prior += np.sum(gamma_logpdf(tau, 1e-5, 1e-5) + np.log(jacobian_softplus(z['tau'])))        \n",
    "        log_prior += np.sum(norm.logpdf(mu, 0, 1.))\n",
    "        log_prior += dirichlet.logpdf(pi, 1e3 * np.ones(self.clusters)) + np.log(jacobian_stick_breaking(z['pi']))\n",
    "        for k in range(self.clusters):\n",
    "            matrix.append(np.log(pi[k]) + np.sum(norm.logpdf(x, mu[(k * self.D):((k + 1) * self.D)],\n",
    "                                np.full([self.D], 1./np.sqrt(tau[k]))), 1))\n",
    "        matrix  = np.vstack(matrix)\n",
    "        vector = logsumexp(matrix, axis=0)\n",
    "        log_lik = np.sum(vector)        \n",
    "        return self.scale * log_lik + log_prior    \n",
    "    \n",
    "    \n",
    "    def predict(self, z):\n",
    "        x = self.data\n",
    "        mu, tau, pi = z['mu'], softplus(z['tau']), stick_breaking(z['pi'])        \n",
    "        matrix = []                \n",
    "        for k in range(self.clusters):\n",
    "            matrix.append(np.log(pi[k]) + np.sum(norm.logpdf(x, mu[(k * self.D):((k + 1) * self.D)],\n",
    "                                 np.full([self.D], 1./np.sqrt(tau[k]))), 1))\n",
    "        matrix  = np.vstack(matrix)                \n",
    "        return np.argmax(matrix, 0)    \n",
    "    \n",
    " \n",
    "    def q_log_prob(self, means, log_sigmas, z):\n",
    "        q_w = np.sum(norm.logpdf(z, means, np.exp(log_sigmas)))        \n",
    "        return q_w\n",
    "      \n",
    "    def q_log_prob_sep(self, means, log_sigmas, z):\n",
    "        q_w = norm.logpdf(z, means, np.exp(log_sigmas))        \n",
    "        return q_w\n",
    "    \n",
    "    def sample_q(self, means, log_sigmas, d):        \n",
    "        eps = npr.randn(d)        \n",
    "        q_s = np.exp(log_sigmas) * eps + means\n",
    "        return (q_s, eps)\n",
    "        \n",
    "    def grad_params(self, dp_log_prob, eps, log_sigmas):                \n",
    "        grad_means = dp_log_prob\n",
    "        grad_log_sigmas = dp_log_prob*eps*np.exp(log_sigmas) + 1                \n",
    "        return np.concatenate([grad_means, grad_log_sigmas])\n",
    "        \n",
    "    def calc_eps(self, means, log_sigma, z):        \n",
    "        eps  = (z - means)/np.exp(log_sigma)\n",
    "        return eps            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Inference(object):      \n",
    "    def __init__(self, model, params):\n",
    "        self.model = model\n",
    "        self.params = params\n",
    "\n",
    "    def run(self, epochs, batch_size, samples, learning_rate, algorithm = 'SGD', optimizer = 'adam'):\n",
    "        epochs = epochs\n",
    "        batches = self.model.N/batch_size\n",
    "        batch_size = batch_size\n",
    "        samples = samples\n",
    "        learning_rate = learning_rate        \n",
    "        \n",
    "        means, unflatten = flatten(self.params['means'])\n",
    "        log_sigmas, unflatten = flatten(self.params['log_sigmas'])        \n",
    "        D =len(means)\n",
    "\n",
    "        self.F = np.zeros(epochs * batches)\n",
    "        self.time = np.zeros(epochs * batches)\n",
    "        adam = Adam(2*D)\n",
    "        f = 0\n",
    "        \n",
    "        grad_p_log_prob = grad(model.p_log_prob, argnum = 1)\n",
    "        grad_q_log_prob = grad(model.q_log_prob, argnum = 1)\n",
    "        \n",
    "        if algorithm == 'SGD':\n",
    "            for e in range(epochs):\n",
    "                for b in range(batches): \n",
    "                    start = time.clock()\n",
    "                    losses = 0.\n",
    "                    d_elbo = 0.\n",
    "                    idx = np.random.choice(np.arange(self.model.N), batch_size, replace=False)                    \n",
    "                    d_elbo = 0.\n",
    "\n",
    "                    for s in range(samples):\n",
    "                        eps = npr.randn(D)        \n",
    "                        z = np.exp(log_sigmas) * eps + means                                            \n",
    "                        p_log_prob = model.p_log_prob(idx, unflatten(z))                        \n",
    "                        dp_log_prob, _ = flatten(grad_p_log_prob(idx, unflatten(z)))\n",
    "                        g =  model.grad_params(dp_log_prob, eps, log_sigmas)                        \n",
    "                        d_elbo += g\n",
    "                        q_log_prob = model.q_log_prob(means, log_sigmas, z)                                         \n",
    "                        losses +=  (p_log_prob - q_log_prob)                    \n",
    "                    loss = losses/samples\n",
    "                    d_elbo /= samples   \n",
    "                    means_old, log_sigmas_old = means, log_sigmas\n",
    "                    means, log_sigmas = adam.update(d_elbo, np.concatenate([means, log_sigmas]), learning_rate)\n",
    "                    if np.sum(np.isnan(means)) > 0 or np.sum(np.isnan(log_sigmas)) > 0:\n",
    "                        means, log_sigmas = means_old, log_sigmas_old\n",
    "                        learning_rate = learning_rate * .1                        \n",
    "                    self.F[f] =  -loss                \n",
    "\n",
    "                    stop = time.clock()\n",
    "                    self.time[f] = stop - start\n",
    "                    f+=1\n",
    "                if e % 1 == 0:\n",
    "                    pstate = 'Epoch = ' + \"{0:0=3d}\".format(e) + ': Loss = {0:.3f}'.format(self.F[f-1])\n",
    "                    print (pstate, end = '\\r')\n",
    "                    sys.stdout.flush()    \n",
    "                    \n",
    "                            \n",
    "        if algorithm == 'iSGD':\n",
    "            n = 1.  \n",
    "            z_old = [0.] * samples\n",
    "            dp_log_prob_old = [0.] * samples\n",
    "            q_log_prob_old = [0.] * samples            \n",
    "\n",
    "            for e in range(epochs):\n",
    "                for b in range(batches): \n",
    "                    start = time.clock()\n",
    "                    losses = 0.\n",
    "                    d_elbo = 0.\n",
    "                    idx = np.random.choice(np.arange(self.model.N), batch_size, replace=False)                                        \n",
    "                    d_elbo = 0.\n",
    "                    if n > .5:\n",
    "                        for s in range(samples):\n",
    "                            eps = npr.randn(D)        \n",
    "                            z = np.exp(log_sigmas) * eps + means\n",
    "                            p_log_prob = model.p_log_prob(idx, unflatten(z))\n",
    "                            q_log_prob = model.q_log_prob_sep(means, log_sigmas, z)                                         \n",
    "                            dp_log_prob, _ = flatten(grad_p_log_prob(idx, unflatten(z)))\n",
    "                            g =  model.grad_params(dp_log_prob, eps, log_sigmas)                        \n",
    "                            d_elbo += g\n",
    "                            losses +=  (p_log_prob - np.sum(q_log_prob))\n",
    "                                        \n",
    "                            z_old[s] = z\n",
    "                            dp_log_prob_old[s] = dp_log_prob\n",
    "                            q_log_prob_old[s] = q_log_prob                            \n",
    "\n",
    "                        loss = losses/samples\n",
    "                        d_elbo /= samples                                            \n",
    "                    else:                         \n",
    "                        for s in range(samples):                            \n",
    "                            eps = (z_old[s] - means)/np.exp(log_sigmas)                            \n",
    "                            q_log_prob = model.q_log_prob_sep(means, log_sigmas, z_old[s])                                         \n",
    "                            w = np.exp(q_log_prob - q_log_prob_old[s])                            \n",
    "                            g =  model.grad_params(w * dp_log_prob_old[s], eps, log_sigmas)                        \n",
    "                            d_elbo += g                            \n",
    "                        d_elbo /= samples                                             \n",
    "                    n = npr.uniform()\n",
    "                    means_old, log_sigmas_old = means, log_sigmas\n",
    "                    means, log_sigmas = adam.update(d_elbo, np.concatenate([means, log_sigmas]), learning_rate)\n",
    "                    if np.sum(np.isnan(means)) > 0 or np.sum(np.isnan(log_sigmas)) > 0:\n",
    "                        means, log_sigmas = means_old, log_sigmas_old\n",
    "                        learning_rate = learning_rate * .9   \n",
    "                        n = 1.\n",
    "                    self.F[f] =  -loss                \n",
    "                    stop = time.clock()\n",
    "                    self.time[f] = stop - start\n",
    "                    f+=1                    \n",
    "                if e % 1 == 0:\n",
    "                    pstate = 'Epoch = ' + \"{0:0=3d}\".format(e) + ': Loss = {0:.3f}'.format(self.F[f-1])\n",
    "                    print (pstate, end = '\\r')\n",
    "                    sys.stdout.flush()  \n",
    "                                        \n",
    "        if algorithm == 'iSRA':\n",
    "            n = 1.  \n",
    "            alpha = .5\n",
    "            z_old = [0.] * samples\n",
    "            dp_log_prob_old = [0.] * samples\n",
    "            q_log_prob_old = [0.] * samples            \n",
    "            d_elbo_avg = np.zeros(2*D)\n",
    "            for e in range(epochs):\n",
    "                for b in range(batches): \n",
    "                    start = time.clock()\n",
    "                    losses = 0.\n",
    "                    d_elbo = 0.\n",
    "                    idx = np.random.choice(np.arange(self.model.N), batch_size, replace=False)                                        \n",
    "                    d_elbo = 0.\n",
    "                    if n > .7:\n",
    "                        for s in range(samples):\n",
    "                            eps = npr.randn(D)        \n",
    "                            z = np.exp(log_sigmas) * eps + means\n",
    "                            p_log_prob = model.p_log_prob(idx, unflatten(z))\n",
    "                            q_log_prob = model.q_log_prob_sep(means, log_sigmas, z)                                         \n",
    "                            dp_log_prob, _ = flatten(grad_p_log_prob(idx, unflatten(z)))\n",
    "                            g =  model.grad_params(dp_log_prob, eps, log_sigmas)                        \n",
    "                            d_elbo += g\n",
    "                            losses +=  (p_log_prob - np.sum(q_log_prob))\n",
    "                                        \n",
    "                            z_old[s] = z\n",
    "                            dp_log_prob_old[s] = dp_log_prob\n",
    "                            q_log_prob_old[s] = q_log_prob                            \n",
    "\n",
    "                        loss = losses/samples\n",
    "                        d_elbo /= samples                                                                    \n",
    "                        d_elbo_avg = .9*d_elbo + (1 - .9)*d_elbo_avg\n",
    "                    else:                         \n",
    "                        for s in range(samples):                            \n",
    "                            eps = (z_old[s] - means)/np.exp(log_sigmas)                            \n",
    "                            q_log_prob = model.q_log_prob_sep(means, log_sigmas, z_old[s])                                         \n",
    "                            w = np.exp(q_log_prob - q_log_prob_old[s])                            \n",
    "                            g =  model.grad_params(w * dp_log_prob_old[s], eps, log_sigmas)                        \n",
    "                            d_elbo += g                            \n",
    "                        d_elbo /= samples                                             \n",
    "                        d_elbo_avg = alpha*d_elbo + (1 - alpha)*d_elbo_avg\n",
    "                    n = npr.uniform()                    \n",
    "                    means_old, log_sigmas_old = means, log_sigmas\n",
    "                    means, log_sigmas = adam.update(d_elbo, np.concatenate([means, log_sigmas]), learning_rate)\n",
    "                    if np.sum(np.isnan(means)) > 0 or np.sum(np.isnan(log_sigmas)) > 0:\n",
    "                        means, log_sigmas = means_old, log_sigmas_old\n",
    "                        learning_rate = learning_rate * .9\n",
    "                        n = 1.                    \n",
    "                    self.F[f] =  -loss                \n",
    "                    stop = time.clock()\n",
    "                    self.time[f] = stop - start\n",
    "                    f+=1                    \n",
    "                if e % 1 == 0:\n",
    "                    pstate = 'Epoch = ' + \"{0:0=3d}\".format(e) + ': Loss = {0:.3f}'.format(self.F[f-1])\n",
    "                    print (pstate, end = '\\r')\n",
    "                    sys.stdout.flush()  \n",
    "                                                     \n",
    "           \n",
    "        if algorithm == 'iSAG':\n",
    "            n = 1.  \n",
    "            z_old = [[0.] * samples for b in range(batches)]            \n",
    "            grad_p_old = [[0.] * samples for b in range(batches)]\n",
    "            q_log_prob_old = [[0.] * samples for b in range(batches)]\n",
    "            p_log_prob_old = [[0.] * samples for b in range(batches)]\n",
    "            for e in range(epochs):\n",
    "                for b in range(batches): \n",
    "                    start = time.clock()\n",
    "                    losses = 0.\n",
    "                    d_elbo = 0.\n",
    "                    idx = np.random.choice(np.arange(self.model.N), batch_size, replace=False)                                        \n",
    "                    d_elbo = 0.                    \n",
    "                    for s in range(samples):\n",
    "                        eps = npr.randn(D)        \n",
    "                        z = np.exp(log_sigmas) * eps + means\n",
    "                        p_log_prob = model.p_log_prob(idx, unflatten(z))\n",
    "                        q_log_prob = model.q_log_prob_sep(means, log_sigmas, z)                                         \n",
    "                        dp_log_prob, _ = flatten(grad_p_log_prob(idx, unflatten(z)))\n",
    "                        g =  model.grad_params(dp_log_prob, eps, log_sigmas)                        \n",
    "                        d_elbo += g\n",
    "                        losses +=  (p_log_prob - np.sum(q_log_prob))\n",
    "                        \n",
    "                        z_old[b][s] = z\n",
    "                        grad_p_old[b][s] = grad_p\n",
    "                        q_log_prob_old[b][s] = q_log_prob\n",
    "                        p_log_prob_old[b][s] = p_log_prob           \n",
    "                    loss = losses/samples\n",
    "                    d_elbo /= samples                                                                                        \n",
    "                    means, log_sigmas = adam.update(d_elbo, np.concatenate([means, log_sigmas]), learning_rate)                    \n",
    "                    self.F[f] =  -loss                \n",
    "                    stop = time.clock()\n",
    "                    self.time[f] = stop - start\n",
    "                    f+=1                    \n",
    "                for b in range(batches):\n",
    "                    for s in range(samples):                            \n",
    "                        eps = (z_old[b][s] - means)/np.exp(log_sigmas)                            \n",
    "                        q_log_prob = model.q_log_prob_sep(means, log_sigmas, z_old[batches-1][s])                                         \n",
    "                        w = np.exp(q_log_prob - q_log_prob_old[b][s])                            \n",
    "\n",
    "                if e % 1 == 0:\n",
    "                    pstate = 'Epoch = ' + \"{0:0=3d}\".format(e) + ': Loss = {0:.3f}'.format(self.F[f-1])\n",
    "                    print (pstate, end = '\\r')\n",
    "                    sys.stdout.flush()  \n",
    "                                          \n",
    "        if algorithm == 'iSAG':\n",
    "            n = 1.  \n",
    "            z_old = [[0.] * self.samples for b in range(self.batches)]            \n",
    "            grad_p_old = [[0.] * self.samples for b in range(self.batches)]\n",
    "            q_log_prob_old = [[0.] * self.samples for b in range(self.batches)]\n",
    "            p_log_prob_old = [[0.] * self.samples for b in range(self.batches)]\n",
    "            \n",
    "            for e in range(epochs):\n",
    "                for b in range(batches): \n",
    "                    start = time.clock()\n",
    "                    losses = 0.\n",
    "                    d_elbo = 0.                    \n",
    "                    idx = np.random.choice(np.arange(self.model.N), batch_size, replace=False)                    \n",
    "                    params = unflatten(fparams)                \n",
    "                    for s in range(self.samples):\n",
    "                        z, eps = model.sample_q(params)\n",
    "                        p_log_prob = model.p_log_prob(idx, z)\n",
    "                        q_log_prob = model.q_log_prob_sep(params, z)                                                                                          \n",
    "                        grad_p = grad_p_log_prob(idx, z)\n",
    "                        g,_ =  flatten(model.grad_p(grad_p, eps, params))\n",
    "                        d_elbo += g                            \n",
    "                        losses +=  (p_log_prob - np.sum(q_log_prob))                        \n",
    "\n",
    "                        z_old[b][s] = z\n",
    "                        grad_p_old[b][s] = grad_p\n",
    "                        q_log_prob_old[b][s] = q_log_prob\n",
    "                        p_log_prob_old[b][s] = p_log_prob                                    \n",
    "                    loss = losses/self.samples\n",
    "                    d_elbo /= self.samples\n",
    "                    step = adam.step(d_elbo)\n",
    "                    fparams += step                        \n",
    "                    self.F[f] =  -loss                \n",
    "                    stop = time.clock()\n",
    "                    p.append(unflatten(np.copy(fparams)))\n",
    "\n",
    "                    self.time[f] = stop - start\n",
    "                    f+=1                       \n",
    "                print(\" \")      \n",
    "                for t in range(batches):\n",
    "                    for s in range(samples):\n",
    "                        eps_ = model.calc_eps(z_old[t][s], params) \n",
    "                        q_log_prob = model.q_log_prob_sep(params, z_old[t][s])                                      \n",
    "                        w = np.exp(q_log_prob - q_log_prob_old[t][s]) \n",
    "                        print(np.mean(w))\n",
    "                if e % 10 == 0:\n",
    "                    pstate = 'Epoch = ' + \"{0:0=3d}\".format(e) + ': Loss = {0:.3f}'.format(self.F[f-1])\n",
    "                    print (pstate, end = '\\r')\n",
    "                    sys.stdout.flush()    \n",
    "        self.params = {'means': unflatten(means), 'log_sigmas': unflatten(log_sigmas)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "N = n_samples = 10000\n",
    "epochs = 10\n",
    "samples = 1\n",
    "learning_rate = 0.1\n",
    "K = clusters = 10\n",
    "seed = 222\n",
    "batch_size = 2000\n",
    "D = 2\n",
    "\n",
    "\n",
    "npr.seed(seed)\n",
    "c =  5 * npr.randn(K * D).reshape([K,D])\n",
    "m = np.tile(c, (N/K,1))\n",
    "X =  npr.randn(N * D).reshape([N,D]) + m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Inference' object has no attribute 'batches'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7ecb071b6fc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'log_sigmas'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'mu'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tau'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pi'\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mnpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0minference\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0minference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iSAG'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolor_iter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'means'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-e907892bd064>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, epochs, batch_size, samples, learning_rate, algorithm, optimizer)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0malgorithm\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'iSAG'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0mz_old\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0mgrad_p_old\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mq_log_prob_old\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Inference' object has no attribute 'batches'"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABrQAAAaACAYAAAApQ7zwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAACKJmgBAAAAAABQtH8BdSS5enQxLkAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3b2461fad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "epochs = 10\n",
    "samples = 1\n",
    "learning_rate = .1\n",
    "seed = 111\n",
    "data = X\n",
    "    \n",
    "N = data.shape[0]\n",
    "D = data.shape[1]\n",
    "model = GMM(data, K, N/batch_size)\n",
    "\n",
    "        \n",
    "f, (ax1, ax2, ax3) = plt.subplots(3,1,figsize=(30,30))\n",
    "\n",
    "\n",
    "npr.seed(seed)    \n",
    "params = {}\n",
    "params['means'] = {'mu': np.ones(K*D), 'tau': npr.randn(K), 'pi':  npr.randn(K-1)}\n",
    "params['log_sigmas'] = {'mu': npr.randn(K*D), 'tau': npr.randn(K), 'pi':  npr.randn(K-1)}\n",
    "inference = Inference(model, params)\n",
    "inference.run(epochs, batch_size, samples, learning_rate, 'iSAG')\n",
    "ax1.plot(np.cumsum(inference.time), -inference.F, color = color_iter.next())\n",
    "p = model.predict(inference.params['means'])\n",
    "means_ = inference.params['means']['mu'].reshape([clusters, D])\n",
    "covariances_ = 1/np.sqrt(np.exp(inference.params['means']['tau']))\n",
    "pi_ = stick_breaking(inference.params['means']['pi'])\n",
    "plot_results(ax2, X, p, means_, covariances_, 0, 'Bayesian GMM')\n",
    "\n",
    "\n",
    "# npr.seed(seed)    \n",
    "# params = {}\n",
    "# params['means'] = {'mu': np.ones(K*D), 'tau': npr.randn(K), 'pi':  npr.randn(K-1)}\n",
    "# params['log_sigmas'] = {'mu': npr.randn(K*D), 'tau': npr.randn(K), 'pi':  npr.randn(K-1)}\n",
    "# inference = Inference(model, params)\n",
    "# inference.run(epochs, batch_size, samples, learning_rate, 'iSGD')\n",
    "# ax1.plot(np.cumsum(inference.time), -inference.F, color = color_iter.next())\n",
    "# p = model.predict(inference.params['means'])\n",
    "# means_ = inference.params['means']['mu'].reshape([clusters, D])\n",
    "# covariances_ = 1/np.sqrt(np.exp(inference.params['means']['tau']))\n",
    "# pi_ = stick_breaking(inference.params['means']['pi'])\n",
    "# plot_results(ax3, X, p, means_, covariances_, 0, 'Bayesian GMM')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
