{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import argparse, gzip, cPickle, sys, time, itertools\n",
    "\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "import autograd.scipy.stats.norm as norm\n",
    "import autograd.scipy.stats.dirichlet as dirichlet\n",
    "from autograd.scipy.misc import logsumexp\n",
    "from autograd.util import flatten_func, flatten\n",
    "from autograd import grad, primitive\n",
    "from autograd.numpy.numpy_grads import unbroadcast\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import mixture\n",
    "\n",
    "from  autograd.scipy.special import gammaln, digamma, gamma\n",
    "from scipy import linalg\n",
    "from scipy import stats, integrate\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.animation as animation\n",
    "import pandas as pd\n",
    "import six\n",
    "from data import load_mnist\n",
    "\n",
    "color_names =  [\"windows blue\",\n",
    "               \"red\",\n",
    "               \"gold\",\n",
    "               \"grass green\",\n",
    "               \"orange\", \"yellow\", \"cornflower\"]\n",
    "colors = sns.xkcd_palette(color_names)\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "class Adam(object):\n",
    "    def __init__(self, dparam, learning_rate = 1e-1, b1=0.9, b2=0.999, eps=10**-8,\n",
    "                         decay_rate = 0.9, decay_steps = 100):                    \n",
    "        self.b1 = b1;\n",
    "        self.b2 = b2;\n",
    "        self.eps = eps        \n",
    "        self.m = np.zeros(dparam)\n",
    "        self.v = np.zeros(dparam)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.i = 0\n",
    "        self.decay_rate = decay_rate\n",
    "        self.decay_steps = decay_steps\n",
    "    \n",
    "    def update(self, gradients, params):        \n",
    "        self.i = self.i+1\n",
    "        step_size = self.learning_rate #* self.decay_rate**(self.i/self.decay_steps)        \n",
    "        self.m = (1 - self.b1) * gradients + self.b1 * self.m\n",
    "        self.v = (1 - self.b2) * (gradients**2) + self.b2 * self.v\n",
    "        mhat = self.m / (1 - self.b1**(self.i))\n",
    "        vhat = self.v / (1 - self.b2**(self.i))                        \n",
    "        params = params + step_size*mhat/(np.sqrt(vhat) + self.eps)        \n",
    "        return np.split(params,2)\n",
    "\n",
    "    \n",
    "@primitive\n",
    "def softplus(x):\n",
    "    return np.log(1. + np.exp(x))\n",
    "\n",
    "softplus.defvjp(lambda g, ans, vs, gvs, x: unbroadcast(vs, gvs, g * 1./(1. + np.exp(-x))))\n",
    "\n",
    "def jacobian_softplus(x):\n",
    "    return 1./(1. + np.exp(-x))\n",
    "\n",
    "relu = lambda x: np.maximum(x, 0.)\n",
    "    \n",
    "@primitive\n",
    "def gamma_logpdf(x, alpha = 1., beta = 1.):\n",
    "    return  (alpha*np.log(beta) + (alpha - 1)*np.log(x) - x*beta - gammaln(alpha))\n",
    "\n",
    "gamma_logpdf.defvjp(lambda g, ans, vs, gvs, x, alpha=1.0, beta=1.0: unbroadcast(vs, gvs, g * ((alpha-1)/x - beta)))\n",
    "gamma_logpdf.defvjp(lambda g, ans, vs, gvs, x, alpha=1.0, beta=1.0: unbroadcast(vs, gvs,  g * (np.log(beta) + np.log(x) - digamma(alpha))), argnum=1)\n",
    "gamma_logpdf.defvjp(lambda g, ans, vs, gvs, x, alpha=1.0, beta=1.0: unbroadcast(vs, gvs,  g * (alpha/beta - x)), argnum=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def unpack_layers(weights):\n",
    "        num_weight_sets = len(weights)\n",
    "        for m, n in shapes:\n",
    "            yield weights[:m*n]     .reshape((m, n)),\\\n",
    "                  weights[m*n:m*n+n].reshape((1, n))\n",
    "            weights = weights[(m+1)*n:]\n",
    "            \n",
    "            \n",
    "class BayesianNeuralNet(object):\n",
    "    def __init__(self, data, test, scale = 1.):\n",
    "        self.data = data     \n",
    "        self.test = test        \n",
    "        self.scale = scale\n",
    "        self.noise_variance = 0.01\n",
    "        self.N = data['x'].shape[0]        \n",
    "        \n",
    "    def predictions(self, weights, inputs):        \n",
    "        for W, b in unpack_layers(weights):\n",
    "            outputs = np.dot(inputs, W) + b\n",
    "            inputs = relu(outputs)                \n",
    "        return outputs - logsumexp(outputs, axis=1, keepdims=True)            \n",
    "    \n",
    "    def p_log_prob(self, idx, weights):\n",
    "        inputs, targets = self.data['x'][idx], self.data['y'][idx]\n",
    "        log_prior = np.sum(norm.logpdf(weights, 0., 1.))\n",
    "        log_lik = np.sum(self.predictions(weights, inputs) * targets)\n",
    "        return self.scale * log_lik + log_prior \n",
    "                                \n",
    "    def q_log_prob(self, means, log_sigmas, z):\n",
    "        q_w = np.sum(norm.logpdf(z, means, np.exp(log_sigmas)))        \n",
    "        return q_w\n",
    "      \n",
    "    def q_log_prob_sep(self, means, log_sigmas, z):\n",
    "        q_w = norm.logpdf(z, means, np.exp(log_sigmas))        \n",
    "        return q_w\n",
    "    \n",
    "    def grad_params(self, dp_log_prob, eps, log_sigmas):                \n",
    "        grad_means = dp_log_prob\n",
    "        grad_log_sigmas = dp_log_prob*eps*np.exp(log_sigmas) + 1                \n",
    "        return np.concatenate([grad_means, grad_log_sigmas])\n",
    "        \n",
    "    def accuracy(self, weights):\n",
    "        inputs, targets = self.test['x'], self.test['y']\n",
    "        target_class    = np.argmax(targets, axis=1)\n",
    "        predicted_class = np.argmax(self.predictions(weights, inputs), axis=1)\n",
    "        return np.mean(predicted_class == target_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Inference(object):      \n",
    "    def __init__(self, model, params):\n",
    "        self.model = model\n",
    "        self.params = params\n",
    "\n",
    "    def run(self, epochs, batch_size, samples, learning_rate, algorithm = 'SGD', optimizer = 'adam'):\n",
    "        epochs = epochs\n",
    "        batches = self.model.N/batch_size        \n",
    "        batch_size = batch_size\n",
    "        samples = samples\n",
    "        learning_rate = learning_rate        \n",
    "        \n",
    "        means, unflatten = flatten(self.params['means'])\n",
    "        log_sigmas, unflatten = flatten(self.params['log_sigmas'])        \n",
    "        D =len(means)\n",
    "\n",
    "        self.F = np.zeros(epochs * batches)\n",
    "        self.accuracy = np.zeros(epochs * batches)\n",
    "        self.time = np.zeros(epochs * batches)\n",
    "        adam = Adam(2*D, learning_rate = learning_rate)\n",
    "        f = 0\n",
    "        \n",
    "        grad_p_log_prob = grad(model.p_log_prob, argnum = 1)\n",
    "        grad_q_log_prob = grad(model.q_log_prob, argnum = 1)    \n",
    "        \n",
    "        if algorithm == 'SGD':\n",
    "            for e in range(epochs):\n",
    "                for b in range(batches):                 \n",
    "                    start = time.clock()\n",
    "                    losses = 0.\n",
    "                    d_elbo = 0.\n",
    "                    idx = np.random.choice(np.arange(self.model.N), batch_size, replace=False)                    \n",
    "                    for s in range(samples):\n",
    "                        eps = npr.randn(D)        \n",
    "                        z = np.exp(log_sigmas) * eps + means                           \n",
    "                        p_log_prob = model.p_log_prob(idx, unflatten(z))                        \n",
    "                        dp_log_prob, _ = flatten(grad_p_log_prob(idx, unflatten(z)))\n",
    "                        g =  model.grad_params(dp_log_prob, eps, log_sigmas)                        \n",
    "                        d_elbo += g\n",
    "                        q_log_prob = model.q_log_prob(means, log_sigmas, z)                                         \n",
    "                        losses +=  (p_log_prob - q_log_prob)                    \n",
    "                    loss = losses/samples\n",
    "                    d_elbo /= samples                       \n",
    "                    means, log_sigmas = adam.update(d_elbo, np.concatenate([means, log_sigmas]))                    \n",
    "                    self.F[f] =  -loss                \n",
    "                    stop = time.clock()\n",
    "                    self.time[f] = stop - start\n",
    "                    self.accuracy[f] = model.accuracy(means)\n",
    "                    f+=1                \n",
    "                if e % 10 == 0:            \n",
    "                    pstate = 'Epoch = ' + \"{0:0=3d}\".format(e) + ': Loss = {0:.3f}'.format(self.F[f-1]) + \\\n",
    "                        ': Test accuracy = {0:.3f}'.format(model.accuracy(means))                                        \n",
    "                    print (pstate, end = '\\r')\n",
    "                    sys.stdout.flush()   \n",
    "                else:\n",
    "                    pstate = 'Epoch = ' + \"{0:0=3d}\".format(e) + ': Loss = {0:.3f}'.format(self.F[f-1])                        \n",
    "                    print (pstate, end = '\\r')\n",
    "                    sys.stdout.flush()   \n",
    "                    \n",
    "                                         \n",
    "        if algorithm == 'iSGD':\n",
    "            n = 1.  \n",
    "            z_old = [0.] * samples\n",
    "            dp_log_prob_old = [0.] * samples\n",
    "            q_log_prob_old = [0.] * samples                        \n",
    "            for e in range(epochs):\n",
    "                for b in range(batches): \n",
    "                    start = time.clock()\n",
    "                    losses = 0.\n",
    "                    d_elbo = 0.                    \n",
    "                    if n > .5:\n",
    "                        idx = np.random.choice(np.arange(self.model.N), batch_size, replace=False)                                        \n",
    "                        for s in range(samples):\n",
    "                            eps = npr.randn(D)        \n",
    "                            z = np.exp(log_sigmas) * eps + means\n",
    "                            p_log_prob = model.p_log_prob(idx, unflatten(z))\n",
    "                            q_log_prob = model.q_log_prob_sep(means, log_sigmas, z)                                         \n",
    "                            dp_log_prob, _ = flatten(grad_p_log_prob(idx, unflatten(z)))\n",
    "                            g =  model.grad_params(dp_log_prob, eps, log_sigmas)                        \n",
    "                            d_elbo += g\n",
    "                            losses +=  (p_log_prob - np.sum(q_log_prob))\n",
    "                                        \n",
    "                            z_old[s] = z\n",
    "                            dp_log_prob_old[s] = dp_log_prob\n",
    "                            q_log_prob_old[s] = q_log_prob                          \n",
    "                        loss = losses/samples\n",
    "                        d_elbo /= samples       \n",
    "                    else:\n",
    "                        for s in range(samples):                                                        \n",
    "                            eps = (z_old[s] - means)/np.exp(log_sigmas)                            \n",
    "                            q_log_prob = model.q_log_prob_sep(means, log_sigmas, z_old[s])                                         \n",
    "                            w = np.exp(q_log_prob - q_log_prob_old[s])                            \n",
    "                            g =  model.grad_params(w * dp_log_prob_old[s], eps, log_sigmas)                        \n",
    "                            d_elbo += g  \n",
    "                            #resample = (np.sum(w > .5) < len(w)/2)                            \n",
    "                            #print(np.sum(w > .5))\n",
    "                        d_elbo /= samples                           \n",
    "                    n = npr.uniform()\n",
    "                    means, log_sigmas = adam.update(d_elbo, np.concatenate([means, log_sigmas]))                    \n",
    "                    self.F[f] =  -loss                \n",
    "                    stop = time.clock()\n",
    "                    self.time[f] = stop - start\n",
    "                    self.accuracy[f] = model.accuracy(means)\n",
    "                    f+=1                    \n",
    "                if e % 10 == 0:       \n",
    "                    pstate = 'Epoch = ' + \"{0:0=3d}\".format(e) + ': Loss = {0:.3f}'.format(self.F[f-1]) + \\\n",
    "                        ': Test accuracy = {0:.3f}'.format(model.accuracy(means))                                            \n",
    "                    print (pstate, end = '\\r')\n",
    "                    sys.stdout.flush()  \n",
    "                else:\n",
    "                    pstate = 'Epoch = ' + \"{0:0=3d}\".format(e) + ': Loss = {0:.3f}'.format(self.F[f-1])                        \n",
    "                    print (pstate, end = '\\r')\n",
    "                    sys.stdout.flush()   \n",
    "        self.params = {'means': unflatten(means), 'log_sigmas': unflatten(log_sigmas)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "layer_sizes = [784, 200, 50, 10]\n",
    "L2_reg = 1.0\n",
    "shapes = list(zip(layer_sizes[:-1], layer_sizes[1:]))\n",
    "num_weights = sum((m+1)*n for m, n in shapes)\n",
    "\n",
    "# Training parameters\n",
    "param_scale = 0.1\n",
    "batch_size = 256\n",
    "num_epochs = 5\n",
    "step_size = 0.001\n",
    "\n",
    "N, train_images, train_labels, test_images, test_labels = load_mnist()\n",
    "N = 10000\n",
    "data, test = {},{}\n",
    "data['x'] = train_images[:N]\n",
    "data['y'] = train_labels[:N]\n",
    "test['x'] = test_images[:1000]\n",
    "test['y'] = test_labels[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 019: Loss = 423307.001: Test accuracy = 0.944\r"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "samples = 1\n",
    "batch_size = 1000\n",
    "learning_rate = 0.01\n",
    "seed = 111\n",
    "model = BayesianNeuralNet(data, test, N/batch_size)\n",
    "f, (ax1, ax2) = plt.subplots(2,1,figsize=(30,30))\n",
    "\n",
    "npr.seed(seed)\n",
    "params = {}\n",
    "params['means']  = 0.1 * npr.randn(num_weights)\n",
    "params['log_sigmas'] = -5 * np.ones(num_weights)\n",
    "inference = Inference(model, params)\n",
    "inference.run(epochs, batch_size, samples, learning_rate, 'iSGD')\n",
    "ax1.plot(np.cumsum(inference.time), -inference.F, color = 'r')\n",
    "ax2.plot(np.cumsum(inference.time), inference.accuracy, color = colors[1])\n",
    "\n",
    "npr.seed(seed)\n",
    "params = {}\n",
    "params['means']  = 0.1 * npr.randn(num_weights)\n",
    "params['log_sigmas'] = -5 * np.ones(num_weights)\n",
    "inference = Inference(model, params)\n",
    "inference.run(epochs, batch_size, samples, learning_rate, 'SGD')\n",
    "ax1.plot(np.cumsum(inference.time), -inference.F, color = 'k')\n",
    "ax2.plot(np.cumsum(inference.time), inference.accuracy, color = colors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
