{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import argparse, gzip, cPickle, sys, time, itertools\n",
    "\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "import autograd.scipy.stats.norm as norm\n",
    "import autograd.scipy.stats.dirichlet as dirichlet\n",
    "from autograd.scipy.misc import logsumexp\n",
    "from autograd.util import flatten_func, flatten\n",
    "from autograd import grad, primitive\n",
    "from autograd.numpy.numpy_grads import unbroadcast\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import mixture\n",
    "\n",
    "from  autograd.scipy.special import gammaln, digamma, gamma\n",
    "from scipy import linalg\n",
    "from scipy import stats, integrate\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.animation as animation\n",
    "import pandas as pd\n",
    "import six\n",
    "\n",
    "\n",
    "\n",
    "color_names =  [\"windows blue\",\n",
    "               \"red\",\n",
    "               \"gold\",\n",
    "               \"grass green\",\n",
    "               \"orange\", \"yellow\", \"cornflower\", \"dark red\", \"dark blue\", \"brown\"]\n",
    "colors = sns.xkcd_palette(color_names)\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "color_iter = itertools.cycle(colors)\n",
    "\n",
    "def plot_results(ax, X, Y, means, covariances, index, title):\n",
    "    #splot = ax.subplot(1, 1, 1 + index)\n",
    "    for i, (mean, covar, color) in enumerate(zip(\n",
    "             means, covariances, color_iter)):\n",
    "        v, w = linalg.eigh(np.diag(np.full([2], covar)))\n",
    "        v = 2. * np.sqrt(2.) * np.sqrt(v)\n",
    "        u = w[0] / linalg.norm(w[0])      \n",
    "\n",
    "        if not np.any(Y == i):\n",
    "            continue\n",
    "        ax.scatter(X[Y == i, 0], X[Y == i, 1], 2., color=color)\n",
    "\n",
    "        angle = np.arctan(u[1] / u[0])\n",
    "        angle = 180. * angle / np.pi  # convert to degrees\n",
    "        ell = mpl.patches.Ellipse(mean, v[0], v[1], 180. + angle, color=color)\n",
    "        #ell.set_clip_box(splot.bbox)\n",
    "        ell.set_alpha(0.5)\n",
    "        ax.add_artist(ell)\n",
    "        \n",
    "\n",
    "class Adam(object):\n",
    "    def __init__(self, dparam, b1=0.9, b2=0.999, eps=1,#10**-8,\n",
    "                         decay_rate = 0.9, decay_steps = 100):                    \n",
    "        self.b1 = b1;\n",
    "        self.b2 = b2;\n",
    "        self.eps = eps        \n",
    "        self.m = np.zeros(dparam)\n",
    "        self.v = np.zeros(dparam)\n",
    "        self.i = 0\n",
    "        self.decay_rate = decay_rate\n",
    "        self.decay_steps = decay_steps\n",
    "    \n",
    "    def update(self, gradients, params, learning_rate = 0.1):        \n",
    "        self.i = self.i+1\n",
    "        step_size = learning_rate * self.decay_rate**(self.i/self.decay_steps)        \n",
    "        self.m = (1 - self.b1) * gradients + self.b1 * self.m\n",
    "        self.v = (1 - self.b2) * (gradients**2) + self.b2 * self.v\n",
    "        mhat = self.m / (1 - self.b1**(self.i))\n",
    "        vhat = self.v / (1 - self.b2**(self.i))                        \n",
    "        params = params + step_size*mhat/(np.sqrt(vhat) + self.eps)        \n",
    "        return np.split(params,2)\n",
    "\n",
    "    \n",
    "@primitive\n",
    "def softplus(x):\n",
    "    return np.log(1. + np.exp(x))\n",
    "\n",
    "softplus.defvjp(lambda g, ans, vs, gvs, x: unbroadcast(vs, gvs, g * 1./(1. + np.exp(-x))))\n",
    "\n",
    "def jacobian_softplus(x):\n",
    "    return 1./(1. + np.exp(-x))\n",
    "\n",
    "@primitive\n",
    "def gamma_logpdf(x, alpha = 1., beta = 1.):\n",
    "    return  (alpha*np.log(beta) + (alpha - 1)*np.log(x) - x*beta - gammaln(alpha))\n",
    "\n",
    "gamma_logpdf.defvjp(lambda g, ans, vs, gvs, x, alpha=1.0, beta=1.0: unbroadcast(vs, gvs, g * ((alpha-1)/x - beta)))\n",
    "gamma_logpdf.defvjp(lambda g, ans, vs, gvs, x, alpha=1.0, beta=1.0: unbroadcast(vs, gvs,  g * (np.log(beta) + np.log(x) - digamma(alpha))), argnum=1)\n",
    "gamma_logpdf.defvjp(lambda g, ans, vs, gvs, x, alpha=1.0, beta=1.0: unbroadcast(vs, gvs,  g * (alpha/beta - x)), argnum=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class LinearRegression(object):\n",
    "    def __init__(self, data, scale):\n",
    "        self.data = data             \n",
    "        self.scale = scale\n",
    "        self.N = data['x'].shape[0]\n",
    "        self.D = data['x'].shape[1]        \n",
    "    \n",
    "    def p_log_prob(self, idx, z):\n",
    "        x, y = self.data['x'][idx], self.data['y'][idx]\n",
    "        w, tau, alpha = z['w'], softplus(z['tau']), softplus(z['alpha'])\n",
    "        log_prior = 0.\n",
    "        log_prior += np.sum(gamma_logpdf(tau, 1e-3, 1e-3) + np.log(jacobian_softplus(z['tau'])))        \n",
    "        log_prior += np.sum(gamma_logpdf(alpha, 1e-3, 1e-3) + np.log(jacobian_softplus(z['alpha'])))        \n",
    "        log_prior += np.sum(norm.logpdf(w, 0, 1./np.sqrt(alpha)))        \n",
    "        log_lik = np.sum(norm.logpdf(y, np.matmul(x, w), 1./np.sqrt(tau)))\n",
    "        return self.scale * log_lik + log_prior                \n",
    " \n",
    "    def q_log_prob(self, means, log_sigmas, z):\n",
    "        q_w = np.sum(norm.logpdf(z, means, np.exp(log_sigmas)))        \n",
    "        return q_w\n",
    "      \n",
    "    def q_log_prob_sep(self, means, log_sigmas, z):\n",
    "        q_w = norm.logpdf(z, means, np.exp(log_sigmas))        \n",
    "        return q_w\n",
    "    \n",
    "    def sample_q(self, means, log_sigmas, d):        \n",
    "        eps = npr.randn(d)        \n",
    "        q_s = np.exp(log_sigmas) * eps + means\n",
    "        return (q_s, eps)\n",
    "        \n",
    "    def grad_params(self, dp_log_prob, eps, log_sigmas):                \n",
    "        grad_means = dp_log_prob\n",
    "        grad_log_sigmas = dp_log_prob*eps*np.exp(log_sigmas) + 1                \n",
    "        return np.concatenate([grad_means, grad_log_sigmas])\n",
    "        \n",
    "    def calc_eps(self, means, log_sigma, z):        \n",
    "        eps  = (z - means)/np.exp(log_sigma)\n",
    "        return eps            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Inference(object):      \n",
    "    def __init__(self, model, params):\n",
    "        self.model = model\n",
    "        self.params = params\n",
    "\n",
    "    def run(self, epochs, batch_size, samples, learning_rate, algorithm = 'SGD', optimizer = 'adam'):\n",
    "        epochs = epochs\n",
    "        batches = self.model.N/batch_size\n",
    "        batch_size = batch_size\n",
    "        samples = samples\n",
    "        learning_rate = learning_rate        \n",
    "        \n",
    "        means, unflatten = flatten(self.params['means'])\n",
    "        log_sigmas, unflatten = flatten(self.params['log_sigmas'])        \n",
    "        D =len(means)\n",
    "\n",
    "        self.F = np.zeros(epochs * batches)\n",
    "        self.time = np.zeros(epochs * batches)\n",
    "        adam = Adam(2*D)\n",
    "        f = 0\n",
    "        \n",
    "        grad_p_log_prob = grad(model.p_log_prob, argnum = 1)\n",
    "        grad_q_log_prob = grad(model.q_log_prob, argnum = 1)\n",
    "        \n",
    "        if algorithm == 'SGD':\n",
    "            for e in range(epochs):\n",
    "                for b in range(batches): \n",
    "                    start = time.clock()\n",
    "                    losses = 0.\n",
    "                    d_elbo = 0.\n",
    "                    idx = np.random.choice(np.arange(self.model.N), batch_size, replace=False)                    \n",
    "                    d_elbo = 0.\n",
    "\n",
    "                    for s in range(samples):\n",
    "                        eps = npr.randn(D)        \n",
    "                        z = np.exp(log_sigmas) * eps + means                                            \n",
    "                        p_log_prob = model.p_log_prob(idx, unflatten(z))                        \n",
    "                        dp_log_prob, _ = flatten(grad_p_log_prob(idx, unflatten(z)))\n",
    "                        g =  model.grad_params(dp_log_prob, eps, log_sigmas)                        \n",
    "                        d_elbo += g\n",
    "                        q_log_prob = model.q_log_prob(means, log_sigmas, z)                                         \n",
    "                        losses +=  (p_log_prob - q_log_prob)                    \n",
    "                    loss = losses/samples\n",
    "                    d_elbo /= samples                       \n",
    "                    means, log_sigmas = adam.update(d_elbo, np.concatenate([means, log_sigmas]), learning_rate)                    \n",
    "                    self.F[f] =  -loss                \n",
    "\n",
    "                    stop = time.clock()\n",
    "                    self.time[f] = stop - start\n",
    "                    f+=1\n",
    "                if e % 1 == 0:\n",
    "                    pstate = 'Epoch = ' + \"{0:0=3d}\".format(e) + ': Loss = {0:.3f}'.format(self.F[f-1])\n",
    "                    print (pstate, end = '\\r')\n",
    "                    sys.stdout.flush()    \n",
    "                    \n",
    "                            \n",
    "        if algorithm == 'iSGD':\n",
    "            n = 1.  \n",
    "            z_old = [0.] * samples\n",
    "            dp_log_prob_old = [0.] * samples\n",
    "            q_log_prob_old = [0.] * samples            \n",
    "\n",
    "            for e in range(epochs):\n",
    "                for b in range(batches): \n",
    "                    start = time.clock()\n",
    "                    losses = 0.\n",
    "                    d_elbo = 0.\n",
    "                    idx = np.random.choice(np.arange(self.model.N), batch_size, replace=False)                                                            \n",
    "                    if n > .7:\n",
    "                        for s in range(samples):\n",
    "                            eps = npr.randn(D)        \n",
    "                            z = np.exp(log_sigmas) * eps + means\n",
    "                            p_log_prob = model.p_log_prob(idx, unflatten(z))\n",
    "                            q_log_prob = model.q_log_prob_sep(means, log_sigmas, z)                                         \n",
    "                            dp_log_prob, _ = flatten(grad_p_log_prob(idx, unflatten(z)))\n",
    "                            g =  model.grad_params(dp_log_prob, eps, log_sigmas)                        \n",
    "                            d_elbo += g\n",
    "                            losses +=  (p_log_prob - np.sum(q_log_prob))\n",
    "                                        \n",
    "                            z_old[s] = z\n",
    "                            dp_log_prob_old[s] = dp_log_prob\n",
    "                            q_log_prob_old[s] = q_log_prob                            \n",
    "\n",
    "                        loss = losses/samples\n",
    "                        d_elbo /= samples                                            \n",
    "                    else:                         \n",
    "                        for s in range(samples):                            \n",
    "                            eps = (z_old[s] - means)/np.exp(log_sigmas)                            \n",
    "                            q_log_prob = model.q_log_prob_sep(means, log_sigmas, z_old[s])                                         \n",
    "                            w = np.exp(q_log_prob - q_log_prob_old[s])                            \n",
    "                            g =  model.grad_params(w * dp_log_prob_old[s], eps, log_sigmas)                        \n",
    "                            d_elbo += g                            \n",
    "                        d_elbo /= samples                                             \n",
    "                    n = npr.uniform()                    \n",
    "                    means, log_sigmas = adam.update(d_elbo, np.concatenate([means, log_sigmas]), learning_rate)                    \n",
    "                    self.F[f] =  -loss                \n",
    "                    stop = time.clock()\n",
    "                    self.time[f] = stop - start\n",
    "                    f+=1                    \n",
    "                if e % 1 == 0:\n",
    "                    pstate = 'Epoch = ' + \"{0:0=3d}\".format(e) + ': Loss = {0:.3f}'.format(self.F[f-1])\n",
    "                    print (pstate, end = '\\r')\n",
    "                    sys.stdout.flush()  \n",
    "                                        \n",
    "        if algorithm == 'iSRA':\n",
    "            n = 1.  \n",
    "            alpha = .6\n",
    "            z_old = [0.] * samples\n",
    "            dp_log_prob_old = [0.] * samples\n",
    "            q_log_prob_old = [0.] * samples            \n",
    "            d_elbo_avg = 0.\n",
    "            for e in range(epochs):\n",
    "                for b in range(batches): \n",
    "                    start = time.clock()\n",
    "                    losses = 0.\n",
    "                    d_elbo = 0.\n",
    "                    idx = np.random.choice(np.arange(self.model.N), batch_size, replace=False)                                                            \n",
    "                    if n > .7:\n",
    "                        for s in range(samples):\n",
    "                            eps = npr.randn(D)        \n",
    "                            z = np.exp(log_sigmas) * eps + means\n",
    "                            p_log_prob = model.p_log_prob(idx, unflatten(z))\n",
    "                            q_log_prob = model.q_log_prob_sep(means, log_sigmas, z)                                         \n",
    "                            dp_log_prob, _ = flatten(grad_p_log_prob(idx, unflatten(z)))\n",
    "                            g =  model.grad_params(dp_log_prob, eps, log_sigmas)                        \n",
    "                            d_elbo += g\n",
    "                            losses +=  (p_log_prob - np.sum(q_log_prob))\n",
    "                                        \n",
    "                            z_old[s] = z\n",
    "                            dp_log_prob_old[s] = dp_log_prob\n",
    "                            q_log_prob_old[s] = q_log_prob                            \n",
    "\n",
    "                        loss = losses/samples\n",
    "                        d_elbo /= samples                                                                    \n",
    "                        d_elbo_avg = alpha*d_elbo + (1 - alpha)*d_elbo_avg\n",
    "                    else:                         \n",
    "                        for s in range(samples):                            \n",
    "                            eps = (z_old[s] - means)/np.exp(log_sigmas)                            \n",
    "                            q_log_prob = model.q_log_prob_sep(means, log_sigmas, z_old[s])                                         \n",
    "                            w = np.exp(q_log_prob - q_log_prob_old[s])                            \n",
    "                            g =  model.grad_params(w * dp_log_prob_old[s], eps, log_sigmas)                        \n",
    "                            d_elbo += g                            \n",
    "                        d_elbo /= samples                                             \n",
    "                        d_elbo_avg = alpha*d_elbo + (1 - alpha)*d_elbo_avg\n",
    "                    n = npr.uniform()                                        \n",
    "                    means, log_sigmas = adam.update(d_elbo_avg, np.concatenate([means, log_sigmas]), learning_rate)                    \n",
    "                    self.F[f] =  -loss                \n",
    "                    stop = time.clock()\n",
    "                    self.time[f] = stop - start\n",
    "                    f+=1                    \n",
    "                if e % 1 == 0:\n",
    "                    pstate = 'Epoch = ' + \"{0:0=3d}\".format(e) + ': Loss = {0:.3f}'.format(self.F[f-1])\n",
    "                    print (pstate, end = '\\r')\n",
    "                    sys.stdout.flush()  \n",
    "                    \n",
    "        self.params = {'means': unflatten(means), 'log_sigmas': unflatten(log_sigmas)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Parameters\n",
    "N = 10000\n",
    "K = 5\n",
    "D = 1\n",
    "x = 5*npr.randn(N*K).reshape([N,K]) + 10\n",
    "alpha = np.ones(K)\n",
    "#lpha[:5] = #\n",
    "w = npr.normal(0, 1./np.sqrt(alpha))\n",
    "y = np.matmul(x, w) + npr.randn(N)\n",
    "data = {}\n",
    "data['x'] = x\n",
    "data['y'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 099: Loss = 110771.75544\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7feaf3ad52d0>]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEBCAYAAABysL6vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feaf33c63d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "seed = 111\n",
    "learning_rate = 0.1\n",
    "samples = 1\n",
    "epochs = 100\n",
    "\n",
    "model = LinearRegression(data, N/batch_size) \n",
    "sns.set_style(style='white')\n",
    "npr.seed(seed)    \n",
    "params = {}\n",
    "params['means'] = {'w': 10 * npr.randn(K), 'alpha':  npr.randn(K), 'tau': npr.randn(1)}\n",
    "params['log_sigmas'] = {'w': npr.randn(K), 'alpha': .1 * npr.randn(K), 'tau': npr.randn(1)}\n",
    "inference = Inference(model, params)  \n",
    "inference.run(epochs, batch_size, samples, learning_rate, 'SGD')\n",
    "plt.plot(np.cumsum(inference.time), -inference.F, color = colors[2])\n",
    "\n",
    "\n",
    "# model = LinearRegression(data, N/batch_size) \n",
    "# sns.set_style(style='white')\n",
    "# npr.seed(seed)    \n",
    "# params = {}\n",
    "# params['means'] = {'w': npr.randn(K), 'alpha': .1* npr.randn(K), 'tau': npr.randn(1)}\n",
    "# params['log_sigmas'] = {'w': npr.randn(K), 'alpha': .1 * npr.randn(K), 'tau': npr.randn(1)}\n",
    "# inference = Inference(model, params)  \n",
    "# inference.run(epochs, batch_size, samples, learning_rate, 'iSGD')\n",
    "# plt.plot(np.cumsum(inference.time), -inference.F, color = colors[0])\n",
    "\n",
    "# model = LinearRegression(data, N/batch_size) \n",
    "# sns.set_style(style='white')\n",
    "# npr.seed(seed)    \n",
    "# params = {}\n",
    "# params['means'] = {'w': npr.randn(K), 'tau': npr.randn(1)}\n",
    "# params['log_sigmas'] = {'w': npr.randn(K), 'tau': npr.randn(1)}\n",
    "# inference = Inference(model, params)  \n",
    "# inference.run(epochs, batch_size, samples, learning_rate, 'iSRA')\n",
    "# plt.plot(np.cumsum(inference.time), -inference.F, color = colors[1])\n",
    "\n",
    "# npr.seed(seed)    \n",
    "# params = {}\n",
    "# params['means'] = {'mu': np.ones(K*D), 'tau': npr.randn(K), 'pi':  npr.randn(K-1)}\n",
    "# params['log_sigmas'] = {'mu': npr.randn(K*D), 'tau': npr.randn(K), 'pi':  npr.randn(K-1)}\n",
    "# inference = Inference(model, params)\n",
    "# inference.run(epochs, batch_size, samples, learning_rate, 'iSGD')\n",
    "# ax1.plot(np.cumsum(inference.time), -inference.F, color = color_iter.next())\n",
    "# p = model.predict(inference.params['means'])\n",
    "# means_ = inference.params['means']['mu'].reshape([clusters, D])\n",
    "# covariances_ = 1/np.sqrt(np.exp(inference.params['means']['tau']))\n",
    "# pi_ = stick_breaking(inference.params['means']['pi'])\n",
    "# plot_results(ax3, X, p, means_, covariances_, 0, 'Bayesian GMM')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Container object of 5 artists>"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD3CAYAAAAALt/WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADy9JREFUeJzt3V9sU3Ufx/EPba1MqhLilBBlCY31ZhdzEJPFNOPCimIG\niZC0OjcSTdQrJWmMaKSZA0fxTzQhoFGjIAFXRUKcMWImS5bMP5Fq0YqCQbKQTMJAjGkrdN05zwV4\nnqducJCHtv7Y+3W109/p+u3Nmx9n69k027ZtAQCM5an1AACA/w8hBwDDEXIAMBwhBwDD+ar5YqdO\nnVI2m1V9fb28Xm81XxoAjDU+Pq7R0VE1NjZq+vTpE9arGvJsNqv29vZqviQAXDa2bdumBQsWTHi8\nqiGvr693hpk9e3Y1XxoAjHX06FG1t7c7Df27qob8r8sps2fP1o033ljNlwYA453rkjQ/7AQAwxFy\nADAcIQcAwxFyADAcIQcAwxFyADAcIQcAwxFyADBcVT8QhP/PyEitJ7h05syp9QTA5YMdOQAYjpAD\ngOEIOQAYjpADgOEIOQAYjpADgOEIOQAYjpADgOEIOQAYjpADgOEIOQAYjpADgOEIOQAYjpADgOEI\nOQAYjpADgOFc/7CEZVnq6urSgQMH5Pf7tXbtWjU0NDjr3333nZLJpGzb1g033KDnn39efr+/okMD\nAP7LdUfe39+vYrGoVCqleDyuZDLprNm2rdWrV2vdunV699131dLSoiNHjlR0YABAOdcdeTqdVjgc\nliQ1NTUpm806a4cPH9bMmTO1efNm/fzzz2ptbVUwGKzctACACVx35LlcToFAwDn2er0qlUqSpJMn\nT+rbb7/VAw88oLfffltffvmlvvjii8pNCwCYwDXkgUBA+XzeObYsSz7fmY38zJkz1dDQoGAwqCuu\nuELhcLhsxw4AqDzXkDc3N2twcFCSlMlkFAqFnLWbbrpJ+Xxew8PDkqS9e/fq5ptvrtCoAIDJuF4j\nj0QiGhoaUiwWk23b6unpUV9fnwqFgqLRqJ577jnF43HZtq1bb71VCxcurMLYAIC/uIbc4/Gou7u7\n7LH//YFmS0uLduzYceknAwBcED4QBACGI+QAYDhCDgCGI+QAYDhCDgCGI+QAYDhCDgCGI+QAYDhC\nDgCGI+QAYDhCDgCGI+QAYDhCDgCGI+QAYDhCDgCGI+QAYDjXPywB/BuMjNR6gktnzpxaT4DLDTty\nADAcIQcAwxFyADAcIQcAwxFyADAcIQcAw7n++qFlWerq6tKBAwfk9/u1du1aNTQ0OOubN2/W+++/\nr1mzZkmSnn32Wc2bN69yEwMAyriGvL+/X8ViUalUSplMRslkUq+++qqzns1mtX79ejU2NlZ0UADA\n5FxDnk6nFQ6HJUlNTU3KZrNl6z/88INef/11jY6OauHChXrkkUcqMykAYFKu18hzuZwCgYBz7PV6\nVSqVnON77rlHXV1d2rJli9LptAYGBiozKQBgUq4hDwQCyufzzrFlWfL5zmzkbdvWihUrNGvWLPn9\nfrW2tmr//v2VmxYAMIFryJubmzU4OChJymQyCoVCzloul1NbW5vy+bxs29ZXX33FtXIAqDLXa+SR\nSERDQ0OKxWKybVs9PT3q6+tToVBQNBpVPB5XZ2en/H6/Wlpa1NraWrFhuXESAEzkGnKPx6Pu7u6y\nx4LBoPN1W1ub2traLv1kAIALwgeCAMBwhBwADEfIAcBwhBwADEfIAcBwhBwADEfIAcBwhBwADEfI\nAcBwhBwADEfIAcBwhBwADEfIAcBwhBwADEfIAcBwhBwADEfIAcBwhBwADEfIAcBwhBwADEfIAcBw\nhBwADEfIAcBwriG3LEuJRELRaFQdHR0aHh6e9LzVq1frxRdfvOQDAgDOzzXk/f39KhaLSqVSisfj\nSiaTE87p7e3VwYMHKzIgAOD8XEOeTqcVDoclSU1NTcpms2Xr33zzjfbt26doNFqZCQEA5+Ua8lwu\np0Ag4Bx7vV6VSiVJ0rFjx7Rx40YlEonKTQgAOC+f2wmBQED5fN45tixLPt+Zp33yySc6efKkHn74\nYY2OjurUqVOaN2+e7r333spNDAAo4xry5uZmDQwMaPHixcpkMgqFQs5aZ2enOjs7JUk7d+7UL7/8\nQsQBoMpcQx6JRDQ0NKRYLCbbttXT06O+vj4VCgWuiwNVMjJS6wkujTlzaj3B5ck15B6PR93d3WWP\nBYPBCeexEweA2uADQQBgOEIOAIYj5ABgOEIOAIYj5ABgOEIOAIYj5ABgOEIOAIYj5ABgOEIOAIYj\n5ABgOEIOAIYj5ABgOEIOAIYj5ABgOEIOAIYj5ABgOEIOAIYj5ABgOEIOAIYj5ABgOEIOAIYj5ABg\nOEIOAIZzDbllWUokEopGo+ro6NDw8HDZ+u7du7Vs2TItX75cW7ZsqdigAIDJuYa8v79fxWJRqVRK\n8XhcyWTSWRsfH9dLL72kzZs3K5VKafv27frtt98qOjAAoJzP7YR0Oq1wOCxJampqUjabdda8Xq8+\n/vhj+Xw+nThxQpZlye/3V25aAMAErjvyXC6nQCDgHHu9XpVKJefY5/Pp008/1dKlS3Xbbbeprq6u\nMpMCACblGvJAIKB8Pu8cW5Yln698I3/nnXdqcHBQY2Nj2rVr16WfEgBwTq4hb25u1uDgoCQpk8ko\nFAo5a7lcTu3t7SoWi/J4PKqrq5PHwy/CAEA1uV4jj0QiGhoaUiwWk23b6unpUV9fnwqFgqLRqJYs\nWaL29nb5fD7dcsstWrJkSTXmBgCc5Rpyj8ej7u7usseCwaDzdTQaVTQavfSTAQAuCNdBAMBwhBwA\nDEfIAcBwhBwADEfIAcBwrr+1AgC1NDJS6wkunTlzKvN92ZEDgOEIOQAYjpADgOEIOQAYjpADgOEI\nOQAYjpADgOEIOQAYjpADgOEIOQAYjpADgOEIOQAYjpADgOEIOQAYjpADgOEIOQAYjpADgOFc/0KQ\nZVnq6urSgQMH5Pf7tXbtWjU0NDjrH330kbZs2SKv16tQKKSuri55PPz7AADV4lrc/v5+FYtFpVIp\nxeNxJZNJZ+3UqVN65ZVX9M4776i3t1e5XE4DAwMVHRgAUM415Ol0WuFwWJLU1NSkbDbrrPn9fvX2\n9qqurk6SVCqVdOWVV1ZoVADAZFxDnsvlFAgEnGOv16tSqXTmyR6PrrvuOknS1q1bVSgUdPvtt1do\nVADAZFyvkQcCAeXzeefYsiz5fL6y4xdeeEGHDx/Whg0bNG3atMpMCgCYlOuOvLm5WYODg5KkTCaj\nUChUtp5IJHT69Glt2rTJucQCAKge1x15JBLR0NCQYrGYbNtWT0+P+vr6VCgU1NjYqB07dmjBggVa\nsWKFJKmzs1ORSKTigwMAznANucfjUXd3d9ljwWDQ+fqnn3669FMBAC4Yv/ANAIYj5ABgOEIOAIYj\n5ABgOEIOAIYj5ABgOEIOAIYj5ABgOEIOAIYj5ABgOEIOAIYj5ABgOEIOAIYj5ABgOEIOAIYj5ABg\nOEIOAIYj5ABgOEIOAIYj5ABgOEIOAIYj5ABgOEIOAIZzDbllWUokEopGo+ro6NDw8PCEc/7880/F\nYjEdOnSoIkMCAM7NNeT9/f0qFotKpVKKx+NKJpNl699//73a29t15MiRig0JADg315Cn02mFw2FJ\nUlNTk7LZbNl6sVjUxo0bNW/evMpMCAA4L5/bCblcToFAwDn2er0qlUry+c48df78+ZWbDgDgynVH\nHggElM/nnWPLspyIAwBqzzXkzc3NGhwclCRlMhmFQqGKDwUAuHCuW+tIJKKhoSHFYjHZtq2enh71\n9fWpUCgoGo1WY0YAwHm4htzj8ai7u7vssWAwOOG8rVu3XrqpAAAXjA8EAYDhCDkAGI6QA4DhCDkA\nGI6QA4DhCDkAGI6QA4DhCDkAGI6QA4DhCDkAGI6QA4DhCDkAGI6QA4DhCDkAGI6QA4DhCDkAGI6Q\nA4DhCDkAGI6QA4DhCDkAGI6QA4DhCDkAGI6QA4DhXENuWZYSiYSi0ag6Ojo0PDxctr5nzx4tW7ZM\n0WhU7733XsUGBQBMzjXk/f39KhaLSqVSisfjSiaTztrY2JjWrVunt956S1u3blUqldLx48crOjAA\noJzP7YR0Oq1wOCxJampqUjabddYOHTqkuXPn6tprr5UkzZ8/X19//bXuvvvuSb/X+Pi4JOno0aMX\nNeyxYxf1tH8ly/rnz5nK738qv3fp8nn/U/m9Sxf3/qX/NvOvhv6da8hzuZwCgYBz7PV6VSqV5PP5\nlMvldPXVVztrM2bMUC6XO+f3Gh0dlSS1t7df2PQAAMfo6KgaGhomPO4a8kAgoHw+7xxbliWfzzfp\nWj6fLwv73zU2Nmrbtm2qr6+X1+v9R28AAKaq8fFxjY6OqrGxcdJ115A3NzdrYGBAixcvViaTUSgU\nctaCwaCGh4f1+++/66qrrtLevXv10EMPnfN7TZ8+XQsWLLiItwEAU9tkO/G/TLNt2z7fky3LUldX\nlw4ePCjbttXT06P9+/erUCgoGo1qz5492rhxo2zb1rJly7hsAgBV5hpyAMC/Gx8IAgDDEXIAMBwh\nBwDDEfKz3G5FMFXs27dPHR0dtR6jqsbGxvTEE0/o/vvv1/Lly/XZZ5/VeqSqGh8f11NPPaVYLKb7\n7rtPBw8erPVIVXfixAm1trbq0KFDtR7lohDys853K4Kp4o033tAzzzyj06dP13qUqvrwww81c+ZM\nbd++XW+++abWrFlT65GqamBgQJLU29urlStX6uWXX67xRNU1NjamRCKh6dOn13qUi0bIzzrfrQim\nirlz52rDhg21HqPq7rrrLj3++OOSJNu2p9yH1e644w7nH6+RkRFdc801NZ6outavX69YLKbrr7++\n1qNcNEJ+1rluRTCVLFq0yPnU7lQyY8YMBQIB5XI5PfbYY1q5cmWtR6o6n8+nVatWac2aNWpra6v1\nOFWzc+dOzZo1y9nEmYqQn3W+WxHg8vfrr7+qs7NTS5cunVIh+1/JZFK7d+/W6tWrVSgUaj1OVXzw\nwQf6/PPP1dHRoR9//FFPPvmkc08ok1Cqs853KwJc3o4fP64HH3xQiURCLS0ttR6n6nbt2qWjR4/q\n0UcfVV1dnaZNmyaPZ2rs8bZt2+Z83dHRoa6uLtXX19dwootDyM+KRCIaGhpSLBZzbkWAqeG1117T\nH3/8oU2bNmnTpk2Szvzg1+Qffv0TixYt0qpVq9Te3q5SqaSnn356yrz3ywUf0QcAw02N/z8BwGWM\nkAOA4Qg5ABiOkAOA4Qg5ABiOkAOA4Qg5ABjuP2FQJm4FrQXoAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feaf3826690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(np.arange(K),softplus(inference.params['means']['alpha']), alpha=0.1, color='b')            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 12.61480304,  -0.55709769,  -8.35987147,  -4.96407977,   1.15052779])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference.params['means']['w']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.44380324,  0.22508184,  0.1060261 , -1.29771196, -1.57457552])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.67541544,  0.65739964,  0.66155811,  0.67742044,  0.63405342])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softplus(inference.params['means']['alpha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
