{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import argparse, gzip, cPickle, sys, time, itertools\n",
    "\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "import autograd.scipy.stats.norm as norm\n",
    "import autograd.scipy.stats.dirichlet as dirichlet\n",
    "from autograd.scipy.misc import logsumexp\n",
    "from autograd.util import flatten_func, flatten\n",
    "from autograd import grad, primitive\n",
    "from autograd.numpy.numpy_grads import unbroadcast\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import mixture\n",
    "\n",
    "from  autograd.scipy.special import gammaln, digamma, gamma\n",
    "from scipy import linalg\n",
    "from scipy import stats, integrate\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.animation as animation\n",
    "import pandas as pd\n",
    "import six\n",
    "\n",
    "color_names =  [\"windows blue\",\"red\", \"gold\", \"grass green\", \"orange\",\n",
    "                \"yellow\", \"cornflower\", \"dark red\", \"dark blue\", \"brown\"]\n",
    "colors = sns.xkcd_palette(color_names)\n",
    "sns.set_style(\"white\")\n",
    "color_iter = itertools.cycle(colors)\n",
    "\n",
    "def plot_results(ax, X, Y, means, covariances, index, title):    \n",
    "    for i, (mean, covar, color) in enumerate(zip(\n",
    "             means, covariances, color_iter)):\n",
    "        v, w = linalg.eigh(np.diag(np.full([2], covar)))\n",
    "        v = 2. * np.sqrt(2.) * np.sqrt(v)\n",
    "        u = w[0] / linalg.norm(w[0])      \n",
    "\n",
    "        if not np.any(Y == i):\n",
    "            continue\n",
    "        ax.scatter(X[Y == i, 0], X[Y == i, 1], 2., color=color)\n",
    "\n",
    "        angle = np.arctan(u[1] / u[0])\n",
    "        angle = 180. * angle / np.pi  # convert to degrees\n",
    "        ell = mpl.patches.Ellipse(mean, v[0], v[1], 180. + angle, color=color)\n",
    "        #ell.set_clip_box(splot.bbox)\n",
    "        ell.set_alpha(0.5)\n",
    "        ax.add_artist(ell)\n",
    "        \n",
    "\n",
    "class Adam(object):\n",
    "    def __init__(self, dparam, b1=0.9, b2=0.999, eps=1,#10**-8,\n",
    "                         decay_rate = 0.9, decay_steps = 100):                    \n",
    "        self.b1 = b1;\n",
    "        self.b2 = b2;\n",
    "        self.eps = eps        \n",
    "        self.m = np.zeros(dparam)\n",
    "        self.v = np.zeros(dparam)\n",
    "        self.i = 0\n",
    "        self.decay_rate = decay_rate\n",
    "        self.decay_steps = decay_steps\n",
    "    \n",
    "    def update(self, gradients, params, learning_rate = 0.1):        \n",
    "        self.i = self.i+1\n",
    "        step_size = learning_rate * self.decay_rate**(self.i/self.decay_steps)        \n",
    "        self.m = (1 - self.b1) * gradients + self.b1 * self.m\n",
    "        self.v = (1 - self.b2) * (gradients**2) + self.b2 * self.v\n",
    "        mhat = self.m / (1 - self.b1**(self.i))\n",
    "        vhat = self.v / (1 - self.b2**(self.i))                        \n",
    "        params = params + step_size*mhat/(np.sqrt(vhat) + self.eps)        \n",
    "        return np.split(params,2)\n",
    "\n",
    "    \n",
    "@primitive\n",
    "def softplus(x):\n",
    "    return np.log(1. + np.exp(x))\n",
    "\n",
    "softplus.defvjp(lambda g, ans, vs, gvs, x: unbroadcast(vs, gvs, g * 1./(1. + np.exp(-x))))\n",
    "\n",
    "def jacobian_softplus(x):\n",
    "    return 1./(1. + np.exp(-x))\n",
    "\n",
    "@primitive\n",
    "def gamma_logpdf(x, alpha = 1., beta = 1.):\n",
    "    return  (alpha*np.log(beta) + (alpha - 1)*np.log(x) - x*beta - gammaln(alpha))\n",
    "\n",
    "gamma_logpdf.defvjp(lambda g, ans, vs, gvs, x, alpha=1.0, beta=1.0: unbroadcast(vs, gvs, g * ((alpha-1)/x - beta)))\n",
    "gamma_logpdf.defvjp(lambda g, ans, vs, gvs, x, alpha=1.0, beta=1.0: unbroadcast(vs, gvs,  g * (np.log(beta) + np.log(x) - digamma(alpha))), argnum=1)\n",
    "gamma_logpdf.defvjp(lambda g, ans, vs, gvs, x, alpha=1.0, beta=1.0: unbroadcast(vs, gvs,  g * (alpha/beta - x)), argnum=2)\n",
    "\n",
    "\n",
    "cumprod = primitive(np.cumprod)\n",
    "\n",
    "def grad_np_cumprod(g, ans, vs, gvs, x, axis=None):\n",
    "    fx = np.cumprod(x, axis=None)\n",
    "    return np.cumsum((fx * g)[::-1])[::-1].reshape(x.shape) / x\n",
    "\n",
    "cumprod.defvjp(grad_np_cumprod)\n",
    "\n",
    "def invlogit(x, eps=sys.float_info.epsilon):\n",
    "    return (1 - 2 * eps) / (1 + np.exp(-x)) + eps\n",
    "\n",
    "def logit(p):\n",
    "    return np.log(p / (1 - p))\n",
    "\n",
    "def stick_breaking(y_):\n",
    "    y = y_.T\n",
    "    Km1 = y.shape[0]\n",
    "    k = np.arange(Km1)[(slice(None), ) + (None, ) * (y.ndim - 1)]\n",
    "    eq_share = logit(1. / (Km1 + 1 - k).astype(str(y_.dtype)))\n",
    "    z = invlogit(y + eq_share, 1e-3)\n",
    "    yl = np.concatenate([z, np.ones(y[:1].shape)])\n",
    "    yu = np.concatenate([np.ones(y[:1].shape), 1 - z])\n",
    "    S = cumprod(yu, 0)\n",
    "    x = S * yl\n",
    "    return x.T\n",
    "\n",
    "def jacobian_stick_breaking(y_):\n",
    "    y = y_.T\n",
    "    Km1 = y.shape[0]\n",
    "    k = np.arange(Km1)[(slice(None), ) + (None, ) * (y.ndim - 1)]\n",
    "    eq_share = logit(1. / (Km1 + 1 - k).astype(str(y_.dtype)))\n",
    "    yl = y + eq_share\n",
    "    yu = np.concatenate([np.ones(y[:1].shape), 1 - invlogit(yl, 1e-3)])\n",
    "    S = cumprod(yu, 0)\n",
    "    return -np.sum(np.log(S[:-1]) - np.log1p(np.exp(yl)) - np.log1p(np.exp(-yl)), 0).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class LinearRegression(object):\n",
    "    def __init__(self, data, scale):\n",
    "        self.data = data             \n",
    "        self.scale = scale\n",
    "        self.N = data['x'].shape[0]\n",
    "        self.D = data['x'].shape[1]        \n",
    "    \n",
    "    def p_log_prob(self, idx, z):\n",
    "        x, y = self.data['x'][idx], self.data['y'][idx]\n",
    "        w, tau, alpha, b = z['w'], softplus(z['tau']), softplus(z['alpha']), z['b']\n",
    "        log_prior = 0.\n",
    "        log_prior += np.sum(gamma_logpdf(tau, 1e-3, 1e-3) + np.log(jacobian_softplus(z['tau'])))        \n",
    "        log_prior += np.sum(gamma_logpdf(alpha, 1e-3, 1e-3) + np.log(jacobian_softplus(z['alpha'])))        \n",
    "        log_prior += np.sum(norm.logpdf(w, 0, 1./np.sqrt(alpha)))        \n",
    "        log_prior += np.sum(norm.logpdf(b, 0, 1))        \n",
    "        log_lik = np.sum(norm.logpdf(y, np.matmul(x, w) + b, 1./np.sqrt(tau)))\n",
    "        return self.scale * log_lik + log_prior                \n",
    " \n",
    "    def q_log_prob(self, means, log_sigmas, z):\n",
    "        q_w = np.sum(norm.logpdf(z, means, np.exp(log_sigmas)))        \n",
    "        return q_w\n",
    "      \n",
    "    def q_log_prob_sep(self, means, log_sigmas, z):\n",
    "        q_w = norm.logpdf(z, means, np.exp(log_sigmas))        \n",
    "        return q_w\n",
    "    \n",
    "    def sample_q(self, means, log_sigmas, d):        \n",
    "        eps = npr.randn(d)        \n",
    "        q_s = np.exp(log_sigmas) * eps + means\n",
    "        return (q_s, eps)\n",
    "        \n",
    "    def grad_params(self, dp_log_prob, eps, log_sigmas):                \n",
    "        grad_means = dp_log_prob\n",
    "        grad_log_sigmas = dp_log_prob*eps*np.exp(log_sigmas) + 1                \n",
    "        return np.concatenate([grad_means, grad_log_sigmas])\n",
    "        \n",
    "    def calc_eps(self, means, log_sigma, z):        \n",
    "        eps  = (z - means)/np.exp(log_sigma)\n",
    "        return eps            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Inference(object):      \n",
    "    def __init__(self, model, params):\n",
    "        self.model = model\n",
    "        self.params = params\n",
    "\n",
    "    def run(self, epochs, batch_size, samples, learning_rate, algorithm = 'SGD', optimizer = 'adam'):\n",
    "        epochs = epochs\n",
    "        batches = self.model.N/batch_size\n",
    "        batch_size = batch_size\n",
    "        samples = samples\n",
    "        learning_rate = learning_rate        \n",
    "        \n",
    "        means, unflatten = flatten(self.params['means'])\n",
    "        log_sigmas, unflatten = flatten(self.params['log_sigmas'])        \n",
    "        D =len(means)\n",
    "\n",
    "        self.F = np.zeros(epochs * batches)\n",
    "        self.time = np.zeros(epochs * batches)\n",
    "        adam = Adam(2*D)\n",
    "        f = 0\n",
    "        \n",
    "        grad_p_log_prob = grad(model.p_log_prob, argnum = 1)\n",
    "        grad_q_log_prob = grad(model.q_log_prob, argnum = 1)\n",
    "        \n",
    "        if algorithm == 'SGD':\n",
    "            for e in range(epochs):\n",
    "                for b in range(batches): \n",
    "                    start = time.clock()\n",
    "                    losses = 0.\n",
    "                    d_elbo = 0.\n",
    "                    idx = np.random.choice(np.arange(self.model.N), batch_size, replace=False)                    \n",
    "                    d_elbo = 0.\n",
    "\n",
    "                    for s in range(samples):\n",
    "                        eps = npr.randn(D)        \n",
    "                        z = np.exp(log_sigmas) * eps + means                                            \n",
    "                        p_log_prob = model.p_log_prob(idx, unflatten(z))                        \n",
    "                        dp_log_prob, _ = flatten(grad_p_log_prob(idx, unflatten(z)))\n",
    "                        g =  model.grad_params(dp_log_prob, eps, log_sigmas)                        \n",
    "                        d_elbo += g\n",
    "                        q_log_prob = model.q_log_prob(means, log_sigmas, z)                                         \n",
    "                        losses +=  (p_log_prob - q_log_prob)                    \n",
    "                    loss = losses/samples\n",
    "                    d_elbo /= samples   \n",
    "                    means_old, log_sigmas_old = means, log_sigmas\n",
    "                    means, log_sigmas = adam.update(d_elbo, np.concatenate([means, log_sigmas]), learning_rate)\n",
    "                    if np.sum(np.isnan(means)) > 0 or np.sum(np.isnan(log_sigmas)) > 0:\n",
    "                        means, log_sigmas = means_old, log_sigmas_old\n",
    "                        learning_rate = learning_rate * .1                        \n",
    "                    self.F[f] =  -loss                \n",
    "\n",
    "                    stop = time.clock()\n",
    "                    self.time[f] = stop - start\n",
    "                    f+=1\n",
    "                if e % 1 == 0:\n",
    "                    pstate = 'Epoch = ' + \"{0:0=3d}\".format(e) + ': Loss = {0:.3f}'.format(self.F[f-1])\n",
    "                    print (pstate, end = '\\r')\n",
    "                    sys.stdout.flush()    \n",
    "                    \n",
    "                            \n",
    "        if algorithm == 'iSGD':\n",
    "            n = 1.  \n",
    "            z_old = [0.] * samples\n",
    "            dp_log_prob_old = [0.] * samples\n",
    "            q_log_prob_old = [0.] * samples            \n",
    "\n",
    "            for e in range(epochs):\n",
    "                for b in range(batches): \n",
    "                    start = time.clock()\n",
    "                    losses = 0.\n",
    "                    d_elbo = 0.\n",
    "                    idx = np.random.choice(np.arange(self.model.N), batch_size, replace=False)                                        \n",
    "                    d_elbo = 0.\n",
    "                    if n > .9:\n",
    "                        for s in range(samples):\n",
    "                            eps = npr.randn(D)        \n",
    "                            z = np.exp(log_sigmas) * eps + means\n",
    "                            p_log_prob = model.p_log_prob(idx, unflatten(z))\n",
    "                            q_log_prob = model.q_log_prob_sep(means, log_sigmas, z)                                         \n",
    "                            dp_log_prob, _ = flatten(grad_p_log_prob(idx, unflatten(z)))\n",
    "                            g =  model.grad_params(dp_log_prob, eps, log_sigmas)                        \n",
    "                            d_elbo += g\n",
    "                            losses +=  (p_log_prob - np.sum(q_log_prob))\n",
    "                                        \n",
    "                            z_old[s] = z\n",
    "                            dp_log_prob_old[s] = dp_log_prob\n",
    "                            q_log_prob_old[s] = q_log_prob                            \n",
    "\n",
    "                        loss = losses/samples\n",
    "                        d_elbo /= samples                                            \n",
    "                    else:                         \n",
    "                        for s in range(samples):                            \n",
    "                            eps = (z_old[s] - means)/np.exp(log_sigmas)                            \n",
    "                            q_log_prob = model.q_log_prob_sep(means, log_sigmas, z_old[s])                                         \n",
    "                            w = np.exp(q_log_prob - q_log_prob_old[s])                            \n",
    "                            g =  model.grad_params(w * dp_log_prob_old[s], eps, log_sigmas)                        \n",
    "                            d_elbo += g                            \n",
    "                        d_elbo /= samples                                             \n",
    "                    n = npr.uniform()\n",
    "                    means_old, log_sigmas_old = means, log_sigmas\n",
    "                    means, log_sigmas = adam.update(d_elbo, np.concatenate([means, log_sigmas]), learning_rate)\n",
    "                    if np.sum(np.isnan(means)) > 0 or np.sum(np.isnan(log_sigmas)) > 0:\n",
    "                        means, log_sigmas = means_old, log_sigmas_old\n",
    "                        learning_rate = learning_rate * .9   \n",
    "                        n = 1.\n",
    "                    self.F[f] =  -loss                \n",
    "                    stop = time.clock()\n",
    "                    self.time[f] = stop - start\n",
    "                    f+=1                    \n",
    "                if e % 1 == 0:\n",
    "                    pstate = 'Epoch = ' + \"{0:0=3d}\".format(e) + ': Loss = {0:.3f}'.format(self.F[f-1])\n",
    "                    print (pstate, end = '\\r')\n",
    "                    sys.stdout.flush()  \n",
    "                                        \n",
    "        if algorithm == 'iSRA':\n",
    "            n = 1.  \n",
    "            alpha = .8\n",
    "            z_old = [0.] * samples\n",
    "            dp_log_prob_old = [0.] * samples\n",
    "            q_log_prob_old = [0.] * samples            \n",
    "            d_elbo_avg = 0.\n",
    "            for e in range(epochs):\n",
    "                for b in range(batches): \n",
    "                    start = time.clock()\n",
    "                    losses = 0.\n",
    "                    d_elbo = 0.\n",
    "                    idx = np.random.choice(np.arange(self.model.N), batch_size, replace=False)                                        \n",
    "                    d_elbo = 0.\n",
    "                    if n > .7:\n",
    "                        for s in range(samples):\n",
    "                            eps = npr.randn(D)        \n",
    "                            z = np.exp(log_sigmas) * eps + means\n",
    "                            p_log_prob = model.p_log_prob(idx, unflatten(z))\n",
    "                            q_log_prob = model.q_log_prob_sep(means, log_sigmas, z)                                         \n",
    "                            dp_log_prob, _ = flatten(grad_p_log_prob(idx, unflatten(z)))\n",
    "                            g =  model.grad_params(dp_log_prob, eps, log_sigmas)                        \n",
    "                            d_elbo += g\n",
    "                            losses +=  (p_log_prob - np.sum(q_log_prob))\n",
    "                                        \n",
    "                            z_old[s] = z\n",
    "                            dp_log_prob_old[s] = dp_log_prob\n",
    "                            q_log_prob_old[s] = q_log_prob                            \n",
    "\n",
    "                        loss = losses/samples\n",
    "                        d_elbo /= samples                                                                    \n",
    "                        d_elbo_avg = alpha*d_elbo + (1 - alpha)*d_elbo_avg\n",
    "                    else:                         \n",
    "                        for s in range(samples):                            \n",
    "                            eps = (z_old[s] - means)/np.exp(log_sigmas)                            \n",
    "                            q_log_prob = model.q_log_prob_sep(means, log_sigmas, z_old[s])                                         \n",
    "                            w = np.exp(q_log_prob - q_log_prob_old[s])                            \n",
    "                            g =  model.grad_params(w * dp_log_prob_old[s], eps, log_sigmas)                        \n",
    "                            d_elbo += g                            \n",
    "                        d_elbo /= samples                                             \n",
    "                        d_elbo_avg = alpha*d_elbo + (1 - alpha)*d_elbo_avg\n",
    "                    n = npr.uniform()                    \n",
    "                    means_old, log_sigmas_old = means, log_sigmas\n",
    "                    means, log_sigmas = adam.update(d_elbo_avg, np.concatenate([means, log_sigmas]), learning_rate)\n",
    "                    if np.sum(np.isnan(means)) > 0 or np.sum(np.isnan(log_sigmas)) > 0:\n",
    "                        means, log_sigmas = means_old, log_sigmas_old\n",
    "                        learning_rate = learning_rate * .9\n",
    "                        n = 1.                    \n",
    "                    self.F[f] =  -loss                \n",
    "                    stop = time.clock()\n",
    "                    self.time[f] = stop - start\n",
    "                    f+=1                    \n",
    "                if e % 1 == 0:\n",
    "                    pstate = 'Epoch = ' + \"{0:0=3d}\".format(e) + ': Loss = {0:.3f}'.format(self.F[f-1])\n",
    "                    print (pstate, end = '\\r')\n",
    "                    sys.stdout.flush()  \n",
    "                    \n",
    "        self.params = {'means': unflatten(means), 'log_sigmas': unflatten(log_sigmas)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Parameters\n",
    "N = 10000\n",
    "K = 500\n",
    "D = 1\n",
    "x = 5*npr.randn(N*K).reshape([N,K]) + 10\n",
    "alpha = np.ones(K)\n",
    "#lpha[:5] = #\n",
    "w = npr.normal(0, 1./np.sqrt(alpha))\n",
    "y = np.matmul(x, w) + npr.randn(N) \n",
    "data = {}\n",
    "data['x'] = x\n",
    "data['y'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 007: Loss = nan06729076.332\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sakaya/autograd/lib/python2.7/site-packages/ipykernel/__main__.py:14: RuntimeWarning: divide by zero encountered in divide\n",
      "/home/sakaya/autograd/lib/python2.7/site-packages/ipykernel/__main__.py:14: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 029: Loss = 83031433494.1956\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4b19ea0f90>]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEBCAYAAABysL6vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4b1aabab50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "seed = 111\n",
    "learning_rate = 0.1\n",
    "samples = 1\n",
    "epochs = 30\n",
    "\n",
    "model = LinearRegression(data, N/batch_size) \n",
    "sns.set_style(style='white')\n",
    "\n",
    "npr.seed(seed)    \n",
    "params = {}\n",
    "params['means'] = {'w':   2 * npr.randn(K), 'b':   5 * npr.randn(1),  'alpha':  npr.randn(K), 'tau': npr.randn(1)}\n",
    "params['log_sigmas'] = {'w':   npr.randn(K), 'b': 5 *   npr.randn(1),  'alpha':  npr.randn(K), 'tau': npr.randn(1)}\n",
    "inference = Inference(model, params)  \n",
    "inference.run(epochs, batch_size, samples, learning_rate, 'SGD')\n",
    "plt.plot(np.cumsum(inference.time), -inference.F, color = colors[0])\n",
    "\n",
    "npr.seed(seed)    \n",
    "params = {}\n",
    "params['means'] = {'w':   2 * npr.randn(K), 'b':   npr.randn(1),  'alpha':  npr.randn(K), 'tau': npr.randn(1)}\n",
    "params['log_sigmas'] = {'w':   npr.randn(K), 'b':   npr.randn(1),  'alpha':  npr.randn(K), 'tau': npr.randn(1)}\n",
    "inference = Inference(model, params)  \n",
    "inference.run(epochs, batch_size, samples, learning_rate, 'iSGD')\n",
    "plt.plot(np.cumsum(inference.time), -inference.F, color = colors[1])\n",
    "\n",
    "\n",
    "# plt.ylabel('ELBO', fontsize = 15)\n",
    "# plt.xlabel('CPU time', fontsize = 15)\n",
    "# plt.xticks(fontsize = 15)\n",
    "# plt.yticks(fontsize = 15)\n",
    "# plt.legend(loc = 4, fontsize = 15)\n",
    "# plt.savefig('/home/sakaya/MUPI/papers/uai17importance/linea.png', dpi=300, bbox_inches= 'tight')\n",
    "\n",
    "# npr.seed(seed)    \n",
    "# params = {}\n",
    "# params['means'] = {'mu': np.ones(K*D), 'tau': npr.randn(K), 'pi':  npr.randn(K-1)}\n",
    "# params['log_sigmas'] = {'mu': npr.randn(K*D), 'tau': npr.randn(K), 'pi':  npr.randn(K-1)}\n",
    "# inference = Inference(model, params)\n",
    "# inference.run(epochs, batch_size, samples, learning_rate, 'iSRA')\n",
    "# ax1.plot(np.cumsum(inference.time), -inference.F, color = colors[1])\n",
    "# p = model.predict(inference.params['means'])\n",
    "# means_ = inference.params['means']['mu'].reshape([clusters, D])\n",
    "# covariances_ = 1/np.sqrt(np.exp(inference.params['means']['tau']))\n",
    "# pi_ = stick_breaking(inference.params['means']['pi'])\n",
    "# plot_results(ax3, X, p, means_, covariances_, 0, 'Bayesian GMM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Container object of 500 artists>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD3CAYAAAAALt/WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFdY1Q0ULAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4b17af5c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k = softplus(inference.params['means']['w'])\n",
    "plt.bar(range(len(k)), k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
